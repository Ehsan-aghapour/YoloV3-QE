{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece1f781",
   "metadata": {
    "id": "KimMZUVqcJ8_"
   },
   "source": [
    "##### Copyright 2021 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631f971d",
   "metadata": {
    "id": "BlWzg1D9_EhW"
   },
   "source": [
    "# Inspecting Quantization Errors with Quantization Debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d3dec",
   "metadata": {
    "id": "XLoHL19yb-a0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/quantization_debugger\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/performance/quantization_debugger.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/google/imagenet/mobilenet_v3_small_100_224/classification/5\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad1b9b",
   "metadata": {
    "id": "qTEEzJWo_iZ_"
   },
   "source": [
    "### Setup\n",
    "\n",
    "This section prepares libraries, MobileNet v3 model, and test dataset of 100\n",
    "images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a9a462b",
   "metadata": {
    "id": "l7epUDUP_6qo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tf-nightly in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (2.13.0.dev20230222)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (0.30.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (0.4.0)\n",
      "Requirement already satisfied: tf-estimator-nightly~=2.13.0.dev in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (2.13.0.dev2023022209)\n",
      "Requirement already satisfied: setuptools in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (65.5.0)\n",
      "Requirement already satisfied: tb-nightly~=2.13.0.a in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (2.13.0a20230221)\n",
      "Requirement already satisfied: keras-nightly~=2.13.0.dev in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (2.13.0.dev2023022208)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (1.14.1)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (0.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (3.20.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (4.5.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (1.51.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (3.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (0.2.0)\n",
      "Requirement already satisfied: packaging in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (23.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (1.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (15.0.6.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (2.0.7)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (2.2.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (1.23.5)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tf-nightly) (1.16.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from astunparse>=1.6.0->tf-nightly) (0.38.4)\n",
      "Requirement already satisfied: scipy>=1.5 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from jax>=0.3.15->tf-nightly) (1.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (2.2.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (2.28.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from tb-nightly~=2.13.0.a->tf-nightly) (2.16.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.13.0.a->tf-nightly) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.13.0.a->tf-nightly) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.13.0.a->tf-nightly) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.13.0.a->tf-nightly) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.13.0.a->tf-nightly) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.13.0.a->tf-nightly) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.13.0.a->tf-nightly) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from requests<3,>=2.21.0->tb-nightly~=2.13.0.a->tf-nightly) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from werkzeug>=1.0.1->tb-nightly~=2.13.0.a->tf-nightly) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.13.0.a->tf-nightly) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.13.0.a->tf-nightly) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (3.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ehsan/Accuracy/YOLOV3/env_qe/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to script Q.ipynb\n",
    "###!jupytext --set-formats ipynb,py Q.py --sync\n",
    "# Quantization debugger is available from TensorFlow 2.7.0\n",
    "!pip uninstall -y tensorflow\n",
    "!pip install tf-nightly\n",
    "#!pip install tensorflow_datasets --upgrade  # imagenet_v2 needs latest checksum\n",
    "#!pip install tensorflow_hub\n",
    "\n",
    "!pip install pandas\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb352b4b",
   "metadata": {
    "id": "LLsgiUZe_hIa",
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 14:57:09.501180: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-30 14:57:09.547994: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-30 14:57:09.548829: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-30 14:57:10.364782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalution dir:/home/ehsan/Accuracy/YOLOV3/Evaluation\n",
      "GPU list:[]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#import tensorflow_datasets as tfds\n",
    "#import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.lite.python import convert\n",
    "import pickle\n",
    "import os\n",
    "from tensorflow.lite.python import interpreter as interpreter_wrapper\n",
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "import pandas as pd \n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "cur_dir=os.getcwd()\n",
    "sys.path.append(cur_dir+'/../Evaluation/')\n",
    "import eval_multiThread2 as tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061679ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df=pd.read_csv(\"df.csv\",index_col=0)\\ndf = pd.DataFrame(columns=[\"name\",\"mAP\"])\\ndf.loc[0]=[\"a\", 3.2]\\ndf.loc[1]=[\"b\", 4.1]\\ndf.to_csv(\"test.csv\")\\ndf2=pd.read_csv(\"test.csv\",index_col=0)\\ndf2.loc[2]=[\"c\", 5.1]\\ndf2\\ndf.loc[71]=[\"2\",3]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''df=pd.read_csv(\"df.csv\",index_col=0)\n",
    "df = pd.DataFrame(columns=[\"name\",\"mAP\"])\n",
    "df.loc[0]=[\"a\", 3.2]\n",
    "df.loc[1]=[\"b\", 4.1]\n",
    "df.to_csv(\"test.csv\")\n",
    "df2=pd.read_csv(\"test.csv\",index_col=0)\n",
    "df2.loc[2]=[\"c\", 5.1]\n",
    "df2\n",
    "df.loc[71]=[\"2\",3]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b652680b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "server=1\n",
    "GPU=1\n",
    "\n",
    "p=\"/home/ehsan/UvA/Accuracy/Keras/\"\n",
    "p_server=\"/home/ehsan/Accuracy/\"\n",
    "if server:\n",
    "    p=p_server\n",
    "data_dir = p+\"YOLOV3/Dataset/val2017\"\n",
    "image_size = (608, 608)\n",
    "N=300\n",
    "\n",
    "\n",
    "resdir='Yolo_files/'\n",
    "ModelName=resdir+'Yolov3.h5'\n",
    "QuantizedName=resdir+'YoloV3_quztized.tflite'\n",
    "QSelectiveName=resdir+'YoloV3_selective_quztized.tflite'\n",
    "UQSelectiveName=resdir+'YoloV3_selective_unquztized.tflite'\n",
    "RESULTS_FILE = resdir+'yolov3_debugger_results.csv'\n",
    "RESULTS_FILE_ANALYZED = resdir+'yolov3_debugger_results_analyzed.csv'\n",
    "RESULTS_FILE_Propogate = resdir+'yolov3_debugger_propogate_results.csv'\n",
    "RESULTS_FILE_Propogate_ANALYZED = resdir+'yolov3_debugger_propogate_results_analyzed.csv'\n",
    "DebuggerName=resdir+'Debugger_Yolov3.pkl'\n",
    "DebuggerPropogateName=resdir+'Debugger_Yolov3_propogation.pkl'\n",
    "CalibratedName=resdir+'YoloV3_calibrated.tflite'\n",
    "\n",
    "\n",
    "# Define the input shape and data type\n",
    "input_shape = (1, 608, 608, 3)\n",
    "input_dtype = tf.float32\n",
    "\n",
    "# Define the output shape and data type\n",
    "output_shape = (1,)\n",
    "output_dtype = tf.float32\n",
    "\n",
    "if GPU:\n",
    "    #tf.debugging.set_log_device_placement(True)\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    print(physical_devices)\n",
    "    #tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1391e9",
   "metadata": {
    "lines_to_end_of_cell_marker": 2,
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef amend_input(m=CalibratedName):\\n    interpreter = tf.lite.Interpreter(model_path=m)\\n    interpreter.allocate_tensors()\\n\\n    input_details = interpreter.get_input_details()\\n    input_shape = input_details[0]['shape']\\n    input_shape[1] = 608\\n    input_shape[2] = 608\\n    interpreter.resize_tensor_input(0, input_shape)\\n    interpreter.allocate_tensors()\\n    print(interpreter.get_input_details())\\n    converter = tf.lite.TFLiteConverter.from_interpreter(interpreter)\\n    #converter.allow_custom_ops = True  # If you have any custom ops in your model\\n    tflite_model = converter.convert()\\n    with open('modified_model.tflite', 'wb') as f:\\n        f.write(tflite_model)\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(m=ModelName):\n",
    "    model = tf.keras.models.load_model(m)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to load and preprocess each image\n",
    "def preprocess_image(file_path):\n",
    "    # Load the image\n",
    "    image = tf.io.read_file(file_path)\n",
    "    # Decode the JPEG image to a tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Resize the image to the desired size\n",
    "    image = tf.image.resize(image, image_size)\n",
    "    # Normalize the pixel values to the range [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image\n",
    "\n",
    "#train_dataset = tf.data.Dataset.from_tensor_slices((images))\n",
    "#train_dataset=train_dataset.map(process_image)\n",
    "def load_dataset():\n",
    "    # Create a list of file paths to the JPEG images\n",
    "    file_paths = tf.data.Dataset.list_files(data_dir + \"/*.jpg\")\n",
    "    # Use the map() method to apply the preprocessing function to each image\n",
    "    dataset = file_paths.map(preprocess_image)\n",
    "    #dataset = dataset.map(lambda x: {'input_1': x})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "def gen_rep():\n",
    "    train_dataset=prepare_dataset()    \n",
    "    representative_dataset = train_dataset.take(100).batch(1)\n",
    "    return representative_dataset\n",
    "    \n",
    "def representative_dataset(dataset):\n",
    "\tdef _data_gen():\n",
    "\t\tfor data in dataset.batch(1):\n",
    "\t\t\tyield [data['image']]\n",
    "\treturn _data_gen\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#representative_dataset = dataset.take(300).batch(1)\n",
    "def rep(_dataset,n=N):\n",
    "    def representative_dataset():\n",
    "        for img in _dataset.take(n):\n",
    "            #img = tf.cast(img, tf.float32)\n",
    "            yield {'input_1': np.array([img])}\n",
    "            #yield np.array(img)\n",
    "    #return tf.data.Dataset.from_generator(representative_dataset, {'input_1': tf.float32}, {'input_1': tf.TensorShape([1, None, None, 3])})\n",
    "    return representative_dataset\n",
    "\n",
    "def rep2(_dataset,n=N):\n",
    "    def representative_dataset():\n",
    "        for img in _dataset.take(n):\n",
    "            #img = tf.cast(img, tf.float32)\n",
    "            \n",
    "            #img = tf.expand_dims(img, axis=0)\n",
    "            #yield [np.array(img)]\n",
    "            \n",
    "            yield [np.array([img])]\n",
    "    return representative_dataset\n",
    "\n",
    "def rep3(_dataset,n=N):\n",
    "    for img in _dataset.take(n):\n",
    "        yield [np.array([img])]\n",
    "\n",
    "def quantize(model,_dataset,name=QuantizedName):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"quantization...\\n\")\n",
    "    if False and (os.path.isfile(QuantizedName)):\n",
    "        print(f\"loading existed {QuantizedName}\")\n",
    "        with open(name, 'rb') as f:\n",
    "            quantized_model=f.read()\n",
    "    else:\n",
    "        print(f'quantization: producing file {QuantizedName}')\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.representative_dataset = tf.lite.RepresentativeDataset(rep(_dataset))\n",
    "        converter.representative_dataset = rep2(_dataset,N)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        #converter.target_spec.supported_ops = [\n",
    "        #    tf.lite.OpsSet.TFLITE_BUILTINS_INT8_GPU \n",
    "        #]\n",
    "        #converter.inference_input_type = tf.uint8\n",
    "        #converter.inference_output_type = tf.uint8\n",
    "        converter.experimental_enable_resource_variables = True\n",
    "        converter.target_spec.supported_types = [tf.int8]\n",
    "\n",
    "        quantized_model = converter.convert()\n",
    "        open(name, \"wb\").write(quantized_model)\n",
    "    return quantized_model\n",
    "\n",
    "def explore(model,_dataset,debugger_name=DebuggerName):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"explore...\\n\")\n",
    "    if (os.path.isfile(DebuggerName)):\n",
    "        print(f'loading existed file {DebuggerName}')\n",
    "        with open(debugger_name, 'rb') as f:\n",
    "            debugger=pickle.load(f)\n",
    "    elif (os.path.isfile(RESULTS_FILE)):\n",
    "        print(f'explore not required, existed file {RESULTS_FILE}')\n",
    "        return \n",
    "    else:\n",
    "        print(f'explore: producing file {DebuggerName}...')\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        #converter.representative_dataset = rep2(_dataset,N)\n",
    "        converter.representative_dataset = tf.lite.RepresentativeDataset(rep2(_dataset))\n",
    "        # my_debug_dataset should have the same format as my_representative_dataset\n",
    "        #debug_dataset=tf.lite.RepresentativeDataset(rep3(_dataset))\n",
    "        debugger = tf.lite.experimental.QuantizationDebugger(\n",
    "            converter=converter, debug_dataset=rep2(_dataset))\n",
    "        #with open(debugger_name, 'wb') as f:\n",
    "        #    pickle.dump(debugger, f)\n",
    "\n",
    "    return debugger\n",
    "\n",
    "def run_debugger(debugger,res_file):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"run debugger...\\n\")\n",
    "    if not (os.path.isfile(res_file)):\n",
    "        print(f'run_debugger: producing file {res_file}')\n",
    "        debugger.run()\n",
    "        with open(res_file, 'w') as f:\n",
    "            debugger.layer_statistics_dump(f)\n",
    "    else:\n",
    "        print(f'run_debugger: file is existed; {res_file}')\n",
    "\n",
    "def Analyze(res_file=RESULTS_FILE_ANALYZED,t=-.33):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"Analyze...\\n\")\n",
    "    layer_stats = pd.read_csv(res_file)\n",
    "    layer_stats.head()\n",
    "    layer_stats['range'] = 255.0 * layer_stats['scale']\n",
    "    layer_stats['rmse/scale'] = layer_stats.apply(\n",
    "        lambda row: np.sqrt(row['mean_squared_error']) / row['scale'], axis=1)\n",
    "    layer_stats[['op_name', 'range', 'rmse/scale']].head()\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax1.bar(np.arange(len(layer_stats)), layer_stats['range'])\n",
    "    ax1.set_ylabel('range')\n",
    "    ax2 = plt.subplot(122)\n",
    "    ax2.bar(np.arange(len(layer_stats)), layer_stats['rmse/scale'])\n",
    "    ax2.set_ylabel('rmse/scale')\n",
    "    plt.show()\n",
    "    #print(layer_stats[layer_stats['rmse/scale'] > t][['op_name', 'range', 'rmse/scale', 'tensor_name']])\n",
    "    layer_stats.to_csv(res_file,sep=',')\n",
    "\n",
    "def selective_quantize(model,_dataset,t=0.33, p=0.5):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"selective quantization...\\n\")\n",
    "    caching=False\n",
    "    if (os.path.isfile(QSelectiveName)) and caching:\n",
    "        print(f'selective quantization: loading existed file {QSelectiveName}')\n",
    "        with open(QSelectiveName, \"rb\") as f:\n",
    "            selective_quantized_model=f.read()\n",
    "    else:\n",
    "        print(f'selective_quatize: producing file {QSelectiveName}')\n",
    "        layer_stats = pd.read_csv(RESULTS_FILE)\n",
    "        suspected_layers = list(layer_stats[layer_stats['rmse/scale'] > t]['tensor_name'])\n",
    "        nn=len(list(layer_stats['tensor_name']))\n",
    "        print(f'Number of layers:{nn}, suspected:{len(suspected_layers)}, unquantized first {int(p*nn)} layers')\n",
    "        suspected_layers.extend(list(layer_stats[:int(p*nn)]['tensor_name']))\n",
    "        print(len(suspected_layers))\n",
    "        print(len(list(set(suspected_layers))))\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = rep(_dataset,N)\n",
    "        debug_options = tf.lite.experimental.QuantizationDebugOptions(denylisted_nodes=suspected_layers)\n",
    "        debugger = tf.lite.experimental.QuantizationDebugger(\n",
    "            converter=converter,debug_dataset=rep(_dataset,N),debug_options=debug_options)\n",
    "        selective_quantized_model = debugger.get_nondebug_quantized_model()\n",
    "        open(QSelectiveName, \"wb\").write(selective_quantized_model)\n",
    "    return selective_quantized_model\n",
    "\n",
    "def calibrate(model,_dataset,name=CalibratedName):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"calibrate...\\n\")\n",
    "    if (os.path.isfile(CalibratedName)):\n",
    "        print(f'calibrate: loading existing file {CalibratedName}')\n",
    "        with open(name, 'rb') as f:\n",
    "            calibrated_model = f.read()\n",
    "    else:\n",
    "        print(f\"calibrate producing file {CalibratedName}\")\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "        converter.representative_dataset = rep2(_dataset)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter._experimental_calibrate_only = True\n",
    "        converter.inference_input_type = input_dtype\n",
    "        converter.inference_output_type = output_dtype\n",
    "        converter.inference_input_shape = input_shape\n",
    "        converter.inference_output_shape = output_shape\n",
    "        calibrated_model = converter.convert()\n",
    "        open(name, \"wb\").write(calibrated_model)\n",
    "    return calibrated_model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def explore_propogation(calibrated_model,_dataset,debugger_name=DebuggerPropogateName):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"explore propogation...\\n\")\n",
    "    if (os.path.isfile(DebuggerPropogateName)):\n",
    "        print(f'explore propogation: loading existed file {DebuggerPropogateName}')\n",
    "        with open(debugger_name, 'rb') as f:\n",
    "            debugger=pickle.load(f)\n",
    "    elif (os.path.isfile(RESULTS_FILE_Propogate)):\n",
    "        print(f'explore propogate not required, existed file {RESULTS_FILE_Propogate}')\n",
    "        return\n",
    "    else:\n",
    "        print(f\"explore_propogation: Producing file {DebuggerPropogateName}\")\n",
    "        # Note that enable_numeric_verify and enable_whole_model_verify are set.\n",
    "        quantized_model = convert.mlir_quantize(\n",
    "            calibrated_model,\n",
    "            enable_numeric_verify=True,\n",
    "            enable_whole_model_verify=True)\n",
    "        debugger = tf.lite.experimental.QuantizationDebugger(\n",
    "            quant_debug_model_content=quantized_model,\n",
    "            debug_dataset=rep2(_dataset))\n",
    "        #with open(debugger_name, 'wb') as f:\n",
    "        #    pickle.dump(debugger, f)\n",
    "    return debugger\n",
    "\n",
    "def explore_combinations(calibrated_model,suspected_layers=[],t=0.35,name=UQSelectiveName):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"explore combination...\\n\")\n",
    "    if (os.path.isfile(name)):\n",
    "        print(f'explore combinations: loading existed file {name}')\n",
    "        with open(name, 'rb') as f:\n",
    "            selective_quantized_model = f.read()\n",
    "    else:\n",
    "        print(f\"explore_combinations: producing file {name}\")\n",
    "        layer_stats = pd.read_csv(RESULTS_FILE)\n",
    "        suspected_layers.extend(list(layer_stats[layer_stats['rmse/scale'] > t]['tensor_name']))\n",
    "        suspected_layers.extend(list(layer_stats[:10]['tensor_name']))\n",
    "        selective_quantized_model = convert.mlir_quantize(calibrated_model, denylisted_nodes=suspected_layers)\n",
    "        open(name, \"wb\").write(selective_quantized_model)\n",
    "    return selective_quantized_model\n",
    "\n",
    "def explore_combinations2(calibrated_model,suspected_layers=[],name=UQSelectiveName):\n",
    "    print(\"\\n\\n\\n\\n***************************************************\")\n",
    "    print(\"explore combination...\\n\")\n",
    "    caching=False\n",
    "    if (os.path.isfile(name)) and caching:\n",
    "        print(f'explore combinations: loading existed file {name}')\n",
    "        with open(name, 'rb') as f:\n",
    "            selective_quantized_model = f.read()\n",
    "    else:\n",
    "        print(f\"explore_combinations: producing file {name}\")\n",
    "        selective_quantized_model = convert.mlir_quantize(calibrated_model, denylisted_nodes=suspected_layers)\n",
    "        with open(name, \"wb\") as f:\n",
    "            f.write(selective_quantized_model)\n",
    "    return \n",
    "\n",
    "'''\n",
    "def amend_input(m=CalibratedName):\n",
    "    interpreter = tf.lite.Interpreter(model_path=m)\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    input_details = interpreter.get_input_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "    input_shape[1] = 608\n",
    "    input_shape[2] = 608\n",
    "    interpreter.resize_tensor_input(0, input_shape)\n",
    "    interpreter.allocate_tensors()\n",
    "    print(interpreter.get_input_details())\n",
    "    converter = tf.lite.TFLiteConverter.from_interpreter(interpreter)\n",
    "    #converter.allow_custom_ops = True  # If you have any custom ops in your model\n",
    "    tflite_model = converter.convert()\n",
    "    with open('modified_model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "336d7619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    global model,dataset,calibrated_model\n",
    "    model=load_model(m=ModelName)\n",
    "    model.summary()\n",
    "    dataset=load_dataset()\n",
    "    # Print the first 5 images in the dataset\n",
    "    for image in dataset.take(5):\n",
    "        print(image.shape)\n",
    "    #global calibrated_model\n",
    "    calibrated_model=calibrate(model,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f050129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import imp\n",
    "#imp.reload(tt)\n",
    "def evaluate(model_name, pkl_name):\n",
    "    #cmd='cd ../Evaluation; python ../Evaluation/eval_multiThread2.py '+args\n",
    "    #os.system(cmd)\n",
    "    mAP,APs=tt.main([\"--num_threads\",'64',\"--model_path\",model_name,\"--pkl_name\",pkl_name])\n",
    "    return mAP,APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "644fc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_indexes(start_conv=-1,end_conv=-1):\n",
    "    ####\n",
    "    layer_stats = pd.read_csv(RESULTS_FILE)\n",
    "    all_layers=list(layer_stats[:]['tensor_name'])\n",
    "    conv_layers=[]\n",
    "    for i,layer in enumerate(all_layers):\n",
    "        if 'conv' in layer or 'StatefulPartitionedCall' in layer:\n",
    "            conv_layers.append(layer)\n",
    "            \n",
    "    _n=len(conv_layers)\n",
    "    #cases=[ [ conv_layers[start:end+1] for end in range(start,_n) ] for start in range(0,_n) ]\n",
    "    #n_cases = [len(case) for case in cases ]\n",
    "    #N_cases = sum(n_cases)\n",
    "    cases=[  list(range(start,end+1))  for start in range(0,_n) for end in range(start,_n)]\n",
    "    N_cases = len(cases)\n",
    "    print(f'Total layers:{len(all_layers)}  Convs:{len(conv_layers)}  number of cases:{N_cases}')\n",
    "    #flatted_cases=[c for case in cases for c in case]\n",
    "    last_conv_indx=len(conv_layers)-1\n",
    "    for i,case in enumerate(cases):\n",
    "        start_conv=case[0]\n",
    "        end_conv=case[-1]\n",
    "        start_index=all_layers.index(conv_layers[start_conv])\n",
    "        if end_conv==last_conv_indx:\n",
    "            end_index=len(all_layers)-1\n",
    "        else:\n",
    "            end_index=all_layers.index(conv_layers[end_conv+1])\n",
    "        suspend=all_layers[0:start_index]+all_layers[end_index:]\n",
    "        #kk=list(set(all_layers)-set(suspend))\n",
    "        #print(f'quantizing conv layers from {start_conv} to {end_conv}')\n",
    "        #print(f'index {start_index} to {end_index}')\n",
    "        #print(all_layers[start_index:end_index])\n",
    "        #ttt=explore_combinations2(calibrated_model,suspected_layers=ss,name='2-3.tflite')'''\n",
    "        ##next_start_conv=cases[i+1]\n",
    "        ##next_end_conv=cases\n",
    "        yield i,N_cases,start_conv,end_conv,start_index,end_index,suspend\n",
    "\n",
    "\n",
    "\n",
    "# +\n",
    "        \n",
    "def run():\n",
    "    output=os.getcwd()+\"/cases/\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    i=0\n",
    "    pklDatafile=\"DataResults.pkl\"\n",
    "    dffile=\"df.csv\"\n",
    "    if os.path.isfile(dffile):\n",
    "        df=pd.read_csv(dffile,index_col=0)\n",
    "        #i=df.iloc[-1][0]+1\n",
    "        i=len(df)\n",
    "        print(f'Continue {dffile} from index {i}')\n",
    "    else:\n",
    "        response=input(\"Do you want to reset df.csv? yes/*   \")\n",
    "        if response==\"yes\" or response==\"Yes\":\n",
    "            df = pd.DataFrame(columns=[\"name\",\"mAP\"])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    if os.path.isfile(pklDatafile):\n",
    "        with open(pklDatafile,'rb') as f:\n",
    "            Data=pickle.load(f)\n",
    "        print(f'{pklDatafile} is loaded')\n",
    "    else:\n",
    "        response=input(\"Do you want to reset DataResults.pkl? yes/*   \")\n",
    "        if response==\"yes\" or response==\"Yes\":\n",
    "            Data=[]\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    for c in generate_indexes():\n",
    "        '''if c[0]<i:\n",
    "            continue'''\n",
    "        #input(f'i is {i}')\n",
    "        print(\"\\n\\n\\n*****************\\n\\n\\n\")\n",
    "        print(f'Case:{c[0]}/{c[1]}')\n",
    "        print(f'quantizing conv layers from {c[2]} to {c[3]}')\n",
    "        print(f'index {c[4]} to {c[5]}')  \n",
    "        _name=f'{c[2]}-{c[3]}'\n",
    "        if df[df['name']==_name].shape[0]:\n",
    "            print(\"Already evaluated...\")\n",
    "            continue\n",
    "        \n",
    "        m_name=output+_name+'.tflite'\n",
    "        p_name=_name+'.pkl'\n",
    "        \n",
    "        start_time=time.time()\n",
    "        explore_combinations2(calibrated_model,suspected_layers=c[-1],name=m_name)\n",
    "        end_time=time.time()\n",
    "        print(f\"{m_name} Quantization finished time: {end_time-start_time}\")\n",
    "        \n",
    "        mAP,APs=evaluate(model_name=m_name,pkl_name=p_name)\n",
    "        end_time=time.time()\n",
    "        print(f\"{m_name} Evaluation finished time: {end_time-start_time}\")\n",
    "        \n",
    "        os.remove(m_name)\n",
    "        df.loc[c[0]]=[_name,mAP]\n",
    "        dct={\"i\":c[0],\"start_conv\":c[2], \"end_conv\":c[3], \"start_index\":c[4], \"end_index\":c[5],\"mAP\":mAP, \"APs\":APs}\n",
    "        Data.append(dct)\n",
    "        if c[0]%5==0 or True:\n",
    "            df.to_csv(dffile)\n",
    "            with open(pklDatafile,'wb') as f:\n",
    "                pickle.dump(Data,f)\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# # +\n",
    "def generate_indexes_2(start_conv=-1,end_conv=-1):\n",
    "    ####\n",
    "    layer_stats = pd.read_csv(RESULTS_FILE)\n",
    "    all_layers=list(layer_stats[:]['tensor_name'])\n",
    "    conv_layers=[]\n",
    "    #print(all_layers)\n",
    "    for i,layer in enumerate(all_layers):\n",
    "        if 'conv' in layer or 'StatefulPartitionedCall' in layer:\n",
    "            conv_layers.append(layer)\n",
    "            \n",
    "    _n=len(conv_layers)\n",
    "    #cases=[ [ conv_layers[start:end+1] for end in range(start,_n) ] for start in range(0,_n) ]\n",
    "    #n_cases = [len(case) for case in cases ]\n",
    "    #N_cases = sum(n_cases)\n",
    "    #cases = list(itertools.combinations(conv_layers, 2))\n",
    "    _convs=list(range(len(conv_layers)))\n",
    "    cases=list(itertools.combinations(_convs, 1))\n",
    "    cases+=list(itertools.combinations(_convs, 2))\n",
    "    N_cases = len(cases)\n",
    "    print(f'Total layers:{len(all_layers)}  Convs:{len(conv_layers)}  number of cases:{N_cases}')\n",
    "    #flatted_cases=[c for case in cases for c in case]\n",
    "    last_conv_indx=len(conv_layers)-1\n",
    "    last_conv=conv_layers[-1]\n",
    "    for i,case in enumerate(cases):\n",
    "        print(f'case:\\n{case}')\n",
    "        suspend=all_layers[:]\n",
    "        quant=[]\n",
    "        for layer in case:           \n",
    "            start_index=all_layers.index(conv_layers[layer])\n",
    "            if layer==last_conv_indx:\n",
    "                end_index=len(all_layers)-1\n",
    "            else:\n",
    "                end_index=all_layers.index(conv_layers[layer+1])\n",
    "            block=[all_layers[j] for j in range(start_index,end_index)]\n",
    "            quant+=block\n",
    "        print(quant)         \n",
    "        suspend=[elem for elem in all_layers if elem not in quant]\n",
    "        print(len(quant),len(suspend),len(all_layers))\n",
    "        \n",
    "        \n",
    "        yield i,N_cases,tuple(case),suspend\n",
    "\n",
    "\n",
    "# + endofcell=\"------\"\n",
    "# # +\n",
    "def extract_one_two_from_consequtives():\n",
    "    m=pd.read_csv('df.csv',index_col=0)\n",
    "    m['layers'] = m['name'].str.split('-').apply(lambda x: tuple(range(int(x[0]), int(x[1])+1)))\n",
    "    #pd.set_option('display.max_rows',3000)\n",
    "    k=1\n",
    "    m_1=m[m['layers'].apply(len) == 1]\n",
    "    m_2=m[m['layers'].apply(len) == 2]\n",
    "    #m_filtered = m[m['layers'].apply(len).isin([1,2])]\n",
    "    ms=pd.concat([m_1,m_2],ignore_index=True)\n",
    "    ms.to_csv(\"extracted_df.csv\",index=False)\n",
    "    return ms\n",
    "#extract_one_two_from_consequtives()\n",
    "\n",
    "\n",
    "# +\n",
    "#t=extract_one_two_from_consequtives()\n",
    "#t[t['layers'].astype(str)=='(0,)']\n",
    "\n",
    "# + endofcell=\"-------\"\n",
    "def run_2():\n",
    "    output=os.getcwd()+\"/cases/\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    dffile=\"df2.csv\"\n",
    "    if os.path.isfile(dffile):\n",
    "        df=pd.read_csv(dffile,index_col=0)\n",
    "        #i=df.iloc[-1][0]+1\n",
    "        i=len(df)\n",
    "        print(f'Continue {dffile} from index {i}')\n",
    "    else:\n",
    "        response=input(\"Do you want to reset df.csv? yes/*   \")\n",
    "        if response==\"yes\" or response==\"Yes\":\n",
    "            df = pd.DataFrame(columns=[\"name\",\"mAP\"])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    \n",
    "    extracted_df=extract_one_two_from_consequtives()\n",
    "    \n",
    "    for c in generate_indexes_2():\n",
    "        \n",
    "        print(\"\\n\\n\\n*****************\\n\\n\\n\")\n",
    "        print(f'Case:{c[0]}/{c[1]}')\n",
    "        print(f'quantizing conv layers {c[2]}')\n",
    "        _name=f'{c[2]}'\n",
    "        if df[df['name']==_name].shape[0]:\n",
    "            print(\"Already evaluated...\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if extracted_df[extracted_df['layers'].astype(str)==_name].shape[0]:\n",
    "            mAP=extracted_df[extracted_df['layers'].astype(str)==_name]['mAP'].iloc[0]\n",
    "            df.loc[c[0]]=[_name,mAP]\n",
    "            df.to_csv(dffile)\n",
    "            print('extract')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        m_name=output+_name+'.tflite'\n",
    "        p_name=_name+'.pkl'\n",
    "        \n",
    "        start_time=time.time()\n",
    "        explore_combinations2(calibrated_model,suspected_layers=c[-1],name=m_name)\n",
    "        end_time=time.time()\n",
    "        print(f\"{m_name} Quantization finished time: {end_time-start_time}\")\n",
    "        \n",
    "        mAP,APs=evaluate(model_name=m_name,pkl_name=p_name)\n",
    "        end_time=time.time()\n",
    "        print(f\"{m_name} Evaluation finished time: {end_time-start_time}\")\n",
    "        \n",
    "        os.remove(m_name)\n",
    "        df.loc[c[0]]=[_name,mAP]\n",
    "        \n",
    "        if c[0]%5==0 or True:\n",
    "            df.to_csv(dffile)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae42f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, None,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " conv_0 (Conv2D)                (None, None, None,   864         ['input_1[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " bnorm_0 (BatchNormalization)   (None, None, None,   128         ['conv_0[0][0]']                 \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " leaky_0 (LeakyReLU)            (None, None, None,   0           ['bnorm_0[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " zero_padding2d_1 (ZeroPadding2  (None, None, None,   0          ['leaky_0[0][0]']                \n",
      " D)                             32)                                                               \n",
      "                                                                                                  \n",
      " conv_1 (Conv2D)                (None, None, None,   18432       ['zero_padding2d_1[0][0]']       \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " bnorm_1 (BatchNormalization)   (None, None, None,   256         ['conv_1[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " leaky_1 (LeakyReLU)            (None, None, None,   0           ['bnorm_1[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv_2 (Conv2D)                (None, None, None,   2048        ['leaky_1[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " bnorm_2 (BatchNormalization)   (None, None, None,   128         ['conv_2[0][0]']                 \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " leaky_2 (LeakyReLU)            (None, None, None,   0           ['bnorm_2[0][0]']                \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv_3 (Conv2D)                (None, None, None,   18432       ['leaky_2[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " bnorm_3 (BatchNormalization)   (None, None, None,   256         ['conv_3[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " leaky_3 (LeakyReLU)            (None, None, None,   0           ['bnorm_3[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, None, None,   0           ['leaky_1[0][0]',                \n",
      "                                64)                               'leaky_3[0][0]']                \n",
      "                                                                                                  \n",
      " zero_padding2d_2 (ZeroPadding2  (None, None, None,   0          ['add_1[0][0]']                  \n",
      " D)                             64)                                                               \n",
      "                                                                                                  \n",
      " conv_5 (Conv2D)                (None, None, None,   73728       ['zero_padding2d_2[0][0]']       \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_5 (BatchNormalization)   (None, None, None,   512         ['conv_5[0][0]']                 \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_5 (LeakyReLU)            (None, None, None,   0           ['bnorm_5[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_6 (Conv2D)                (None, None, None,   8192        ['leaky_5[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " bnorm_6 (BatchNormalization)   (None, None, None,   256         ['conv_6[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " leaky_6 (LeakyReLU)            (None, None, None,   0           ['bnorm_6[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv_7 (Conv2D)                (None, None, None,   73728       ['leaky_6[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_7 (BatchNormalization)   (None, None, None,   512         ['conv_7[0][0]']                 \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_7 (LeakyReLU)            (None, None, None,   0           ['bnorm_7[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, None, None,   0           ['leaky_5[0][0]',                \n",
      "                                128)                              'leaky_7[0][0]']                \n",
      "                                                                                                  \n",
      " conv_9 (Conv2D)                (None, None, None,   8192        ['add_2[0][0]']                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " bnorm_9 (BatchNormalization)   (None, None, None,   256         ['conv_9[0][0]']                 \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " leaky_9 (LeakyReLU)            (None, None, None,   0           ['bnorm_9[0][0]']                \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " conv_10 (Conv2D)               (None, None, None,   73728       ['leaky_9[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_10 (BatchNormalization)  (None, None, None,   512         ['conv_10[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_10 (LeakyReLU)           (None, None, None,   0           ['bnorm_10[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, None, None,   0           ['add_2[0][0]',                  \n",
      "                                128)                              'leaky_10[0][0]']               \n",
      "                                                                                                  \n",
      " zero_padding2d_3 (ZeroPadding2  (None, None, None,   0          ['add_3[0][0]']                  \n",
      " D)                             128)                                                              \n",
      "                                                                                                  \n",
      " conv_12 (Conv2D)               (None, None, None,   294912      ['zero_padding2d_3[0][0]']       \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_12 (BatchNormalization)  (None, None, None,   1024        ['conv_12[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_12 (LeakyReLU)           (None, None, None,   0           ['bnorm_12[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_13 (Conv2D)               (None, None, None,   32768       ['leaky_12[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_13 (BatchNormalization)  (None, None, None,   512         ['conv_13[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_13 (LeakyReLU)           (None, None, None,   0           ['bnorm_13[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_14 (Conv2D)               (None, None, None,   294912      ['leaky_13[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_14 (BatchNormalization)  (None, None, None,   1024        ['conv_14[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_14 (LeakyReLU)           (None, None, None,   0           ['bnorm_14[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, None, None,   0           ['leaky_12[0][0]',               \n",
      "                                256)                              'leaky_14[0][0]']               \n",
      "                                                                                                  \n",
      " conv_16 (Conv2D)               (None, None, None,   32768       ['add_4[0][0]']                  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_16 (BatchNormalization)  (None, None, None,   512         ['conv_16[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_16 (LeakyReLU)           (None, None, None,   0           ['bnorm_16[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_17 (Conv2D)               (None, None, None,   294912      ['leaky_16[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_17 (BatchNormalization)  (None, None, None,   1024        ['conv_17[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_17 (LeakyReLU)           (None, None, None,   0           ['bnorm_17[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, None, None,   0           ['add_4[0][0]',                  \n",
      "                                256)                              'leaky_17[0][0]']               \n",
      "                                                                                                  \n",
      " conv_19 (Conv2D)               (None, None, None,   32768       ['add_5[0][0]']                  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_19 (BatchNormalization)  (None, None, None,   512         ['conv_19[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_19 (LeakyReLU)           (None, None, None,   0           ['bnorm_19[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_20 (Conv2D)               (None, None, None,   294912      ['leaky_19[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_20 (BatchNormalization)  (None, None, None,   1024        ['conv_20[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_20 (LeakyReLU)           (None, None, None,   0           ['bnorm_20[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, None, None,   0           ['add_5[0][0]',                  \n",
      "                                256)                              'leaky_20[0][0]']               \n",
      "                                                                                                  \n",
      " conv_22 (Conv2D)               (None, None, None,   32768       ['add_6[0][0]']                  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_22 (BatchNormalization)  (None, None, None,   512         ['conv_22[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_22 (LeakyReLU)           (None, None, None,   0           ['bnorm_22[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_23 (Conv2D)               (None, None, None,   294912      ['leaky_22[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_23 (BatchNormalization)  (None, None, None,   1024        ['conv_23[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_23 (LeakyReLU)           (None, None, None,   0           ['bnorm_23[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, None, None,   0           ['add_6[0][0]',                  \n",
      "                                256)                              'leaky_23[0][0]']               \n",
      "                                                                                                  \n",
      " conv_25 (Conv2D)               (None, None, None,   32768       ['add_7[0][0]']                  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_25 (BatchNormalization)  (None, None, None,   512         ['conv_25[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_25 (LeakyReLU)           (None, None, None,   0           ['bnorm_25[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_26 (Conv2D)               (None, None, None,   294912      ['leaky_25[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_26 (BatchNormalization)  (None, None, None,   1024        ['conv_26[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_26 (LeakyReLU)           (None, None, None,   0           ['bnorm_26[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, None, None,   0           ['add_7[0][0]',                  \n",
      "                                256)                              'leaky_26[0][0]']               \n",
      "                                                                                                  \n",
      " conv_28 (Conv2D)               (None, None, None,   32768       ['add_8[0][0]']                  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_28 (BatchNormalization)  (None, None, None,   512         ['conv_28[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_28 (LeakyReLU)           (None, None, None,   0           ['bnorm_28[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_29 (Conv2D)               (None, None, None,   294912      ['leaky_28[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_29 (BatchNormalization)  (None, None, None,   1024        ['conv_29[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_29 (LeakyReLU)           (None, None, None,   0           ['bnorm_29[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_9 (Add)                    (None, None, None,   0           ['add_8[0][0]',                  \n",
      "                                256)                              'leaky_29[0][0]']               \n",
      "                                                                                                  \n",
      " conv_31 (Conv2D)               (None, None, None,   32768       ['add_9[0][0]']                  \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_31 (BatchNormalization)  (None, None, None,   512         ['conv_31[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_31 (LeakyReLU)           (None, None, None,   0           ['bnorm_31[0][0]']               \n",
      "                                128)                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv_32 (Conv2D)               (None, None, None,   294912      ['leaky_31[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_32 (BatchNormalization)  (None, None, None,   1024        ['conv_32[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_32 (LeakyReLU)           (None, None, None,   0           ['bnorm_32[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_10 (Add)                   (None, None, None,   0           ['add_9[0][0]',                  \n",
      "                                256)                              'leaky_32[0][0]']               \n",
      "                                                                                                  \n",
      " conv_34 (Conv2D)               (None, None, None,   32768       ['add_10[0][0]']                 \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_34 (BatchNormalization)  (None, None, None,   512         ['conv_34[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_34 (LeakyReLU)           (None, None, None,   0           ['bnorm_34[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_35 (Conv2D)               (None, None, None,   294912      ['leaky_34[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_35 (BatchNormalization)  (None, None, None,   1024        ['conv_35[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_35 (LeakyReLU)           (None, None, None,   0           ['bnorm_35[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " add_11 (Add)                   (None, None, None,   0           ['add_10[0][0]',                 \n",
      "                                256)                              'leaky_35[0][0]']               \n",
      "                                                                                                  \n",
      " zero_padding2d_4 (ZeroPadding2  (None, None, None,   0          ['add_11[0][0]']                 \n",
      " D)                             256)                                                              \n",
      "                                                                                                  \n",
      " conv_37 (Conv2D)               (None, None, None,   1179648     ['zero_padding2d_4[0][0]']       \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_37 (BatchNormalization)  (None, None, None,   2048        ['conv_37[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_37 (LeakyReLU)           (None, None, None,   0           ['bnorm_37[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_38 (Conv2D)               (None, None, None,   131072      ['leaky_37[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_38 (BatchNormalization)  (None, None, None,   1024        ['conv_38[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_38 (LeakyReLU)           (None, None, None,   0           ['bnorm_38[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_39 (Conv2D)               (None, None, None,   1179648     ['leaky_38[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_39 (BatchNormalization)  (None, None, None,   2048        ['conv_39[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_39 (LeakyReLU)           (None, None, None,   0           ['bnorm_39[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, None, None,   0           ['leaky_37[0][0]',               \n",
      "                                512)                              'leaky_39[0][0]']               \n",
      "                                                                                                  \n",
      " conv_41 (Conv2D)               (None, None, None,   131072      ['add_12[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_41 (BatchNormalization)  (None, None, None,   1024        ['conv_41[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_41 (LeakyReLU)           (None, None, None,   0           ['bnorm_41[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_42 (Conv2D)               (None, None, None,   1179648     ['leaky_41[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_42 (BatchNormalization)  (None, None, None,   2048        ['conv_42[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_42 (LeakyReLU)           (None, None, None,   0           ['bnorm_42[0][0]']               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, None, None,   0           ['add_12[0][0]',                 \n",
      "                                512)                              'leaky_42[0][0]']               \n",
      "                                                                                                  \n",
      " conv_44 (Conv2D)               (None, None, None,   131072      ['add_13[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_44 (BatchNormalization)  (None, None, None,   1024        ['conv_44[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_44 (LeakyReLU)           (None, None, None,   0           ['bnorm_44[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_45 (Conv2D)               (None, None, None,   1179648     ['leaky_44[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_45 (BatchNormalization)  (None, None, None,   2048        ['conv_45[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_45 (LeakyReLU)           (None, None, None,   0           ['bnorm_45[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, None, None,   0           ['add_13[0][0]',                 \n",
      "                                512)                              'leaky_45[0][0]']               \n",
      "                                                                                                  \n",
      " conv_47 (Conv2D)               (None, None, None,   131072      ['add_14[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_47 (BatchNormalization)  (None, None, None,   1024        ['conv_47[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_47 (LeakyReLU)           (None, None, None,   0           ['bnorm_47[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_48 (Conv2D)               (None, None, None,   1179648     ['leaky_47[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_48 (BatchNormalization)  (None, None, None,   2048        ['conv_48[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_48 (LeakyReLU)           (None, None, None,   0           ['bnorm_48[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, None, None,   0           ['add_14[0][0]',                 \n",
      "                                512)                              'leaky_48[0][0]']               \n",
      "                                                                                                  \n",
      " conv_50 (Conv2D)               (None, None, None,   131072      ['add_15[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_50 (BatchNormalization)  (None, None, None,   1024        ['conv_50[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_50 (LeakyReLU)           (None, None, None,   0           ['bnorm_50[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_51 (Conv2D)               (None, None, None,   1179648     ['leaky_50[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_51 (BatchNormalization)  (None, None, None,   2048        ['conv_51[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_51 (LeakyReLU)           (None, None, None,   0           ['bnorm_51[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_16 (Add)                   (None, None, None,   0           ['add_15[0][0]',                 \n",
      "                                512)                              'leaky_51[0][0]']               \n",
      "                                                                                                  \n",
      " conv_53 (Conv2D)               (None, None, None,   131072      ['add_16[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_53 (BatchNormalization)  (None, None, None,   1024        ['conv_53[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_53 (LeakyReLU)           (None, None, None,   0           ['bnorm_53[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_54 (Conv2D)               (None, None, None,   1179648     ['leaky_53[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_54 (BatchNormalization)  (None, None, None,   2048        ['conv_54[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " leaky_54 (LeakyReLU)           (None, None, None,   0           ['bnorm_54[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_17 (Add)                   (None, None, None,   0           ['add_16[0][0]',                 \n",
      "                                512)                              'leaky_54[0][0]']               \n",
      "                                                                                                  \n",
      " conv_56 (Conv2D)               (None, None, None,   131072      ['add_17[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_56 (BatchNormalization)  (None, None, None,   1024        ['conv_56[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_56 (LeakyReLU)           (None, None, None,   0           ['bnorm_56[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_57 (Conv2D)               (None, None, None,   1179648     ['leaky_56[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_57 (BatchNormalization)  (None, None, None,   2048        ['conv_57[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_57 (LeakyReLU)           (None, None, None,   0           ['bnorm_57[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_18 (Add)                   (None, None, None,   0           ['add_17[0][0]',                 \n",
      "                                512)                              'leaky_57[0][0]']               \n",
      "                                                                                                  \n",
      " conv_59 (Conv2D)               (None, None, None,   131072      ['add_18[0][0]']                 \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_59 (BatchNormalization)  (None, None, None,   1024        ['conv_59[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_59 (LeakyReLU)           (None, None, None,   0           ['bnorm_59[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_60 (Conv2D)               (None, None, None,   1179648     ['leaky_59[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_60 (BatchNormalization)  (None, None, None,   2048        ['conv_60[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_60 (LeakyReLU)           (None, None, None,   0           ['bnorm_60[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " add_19 (Add)                   (None, None, None,   0           ['add_18[0][0]',                 \n",
      "                                512)                              'leaky_60[0][0]']               \n",
      "                                                                                                  \n",
      " zero_padding2d_5 (ZeroPadding2  (None, None, None,   0          ['add_19[0][0]']                 \n",
      " D)                             512)                                                              \n",
      "                                                                                                  \n",
      " conv_62 (Conv2D)               (None, None, None,   4718592     ['zero_padding2d_5[0][0]']       \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_62 (BatchNormalization)  (None, None, None,   4096        ['conv_62[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_62 (LeakyReLU)           (None, None, None,   0           ['bnorm_62[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv_63 (Conv2D)               (None, None, None,   524288      ['leaky_62[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_63 (BatchNormalization)  (None, None, None,   2048        ['conv_63[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_63 (LeakyReLU)           (None, None, None,   0           ['bnorm_63[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_64 (Conv2D)               (None, None, None,   4718592     ['leaky_63[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_64 (BatchNormalization)  (None, None, None,   4096        ['conv_64[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_64 (LeakyReLU)           (None, None, None,   0           ['bnorm_64[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, None, None,   0           ['leaky_62[0][0]',               \n",
      "                                1024)                             'leaky_64[0][0]']               \n",
      "                                                                                                  \n",
      " conv_66 (Conv2D)               (None, None, None,   524288      ['add_20[0][0]']                 \n",
      "                                512)                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " bnorm_66 (BatchNormalization)  (None, None, None,   2048        ['conv_66[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_66 (LeakyReLU)           (None, None, None,   0           ['bnorm_66[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_67 (Conv2D)               (None, None, None,   4718592     ['leaky_66[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_67 (BatchNormalization)  (None, None, None,   4096        ['conv_67[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_67 (LeakyReLU)           (None, None, None,   0           ['bnorm_67[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, None, None,   0           ['add_20[0][0]',                 \n",
      "                                1024)                             'leaky_67[0][0]']               \n",
      "                                                                                                  \n",
      " conv_69 (Conv2D)               (None, None, None,   524288      ['add_21[0][0]']                 \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_69 (BatchNormalization)  (None, None, None,   2048        ['conv_69[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_69 (LeakyReLU)           (None, None, None,   0           ['bnorm_69[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_70 (Conv2D)               (None, None, None,   4718592     ['leaky_69[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_70 (BatchNormalization)  (None, None, None,   4096        ['conv_70[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_70 (LeakyReLU)           (None, None, None,   0           ['bnorm_70[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " add_22 (Add)                   (None, None, None,   0           ['add_21[0][0]',                 \n",
      "                                1024)                             'leaky_70[0][0]']               \n",
      "                                                                                                  \n",
      " conv_72 (Conv2D)               (None, None, None,   524288      ['add_22[0][0]']                 \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_72 (BatchNormalization)  (None, None, None,   2048        ['conv_72[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_72 (LeakyReLU)           (None, None, None,   0           ['bnorm_72[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_73 (Conv2D)               (None, None, None,   4718592     ['leaky_72[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_73 (BatchNormalization)  (None, None, None,   4096        ['conv_73[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_73 (LeakyReLU)           (None, None, None,   0           ['bnorm_73[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " add_23 (Add)                   (None, None, None,   0           ['add_22[0][0]',                 \n",
      "                                1024)                             'leaky_73[0][0]']               \n",
      "                                                                                                  \n",
      " conv_75 (Conv2D)               (None, None, None,   524288      ['add_23[0][0]']                 \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_75 (BatchNormalization)  (None, None, None,   2048        ['conv_75[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_75 (LeakyReLU)           (None, None, None,   0           ['bnorm_75[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_76 (Conv2D)               (None, None, None,   4718592     ['leaky_75[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_76 (BatchNormalization)  (None, None, None,   4096        ['conv_76[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_76 (LeakyReLU)           (None, None, None,   0           ['bnorm_76[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv_77 (Conv2D)               (None, None, None,   524288      ['leaky_76[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_77 (BatchNormalization)  (None, None, None,   2048        ['conv_77[0][0]']                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_77 (LeakyReLU)           (None, None, None,   0           ['bnorm_77[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_78 (Conv2D)               (None, None, None,   4718592     ['leaky_77[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_78 (BatchNormalization)  (None, None, None,   4096        ['conv_78[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_78 (LeakyReLU)           (None, None, None,   0           ['bnorm_78[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv_79 (Conv2D)               (None, None, None,   524288      ['leaky_78[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_79 (BatchNormalization)  (None, None, None,   2048        ['conv_79[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_79 (LeakyReLU)           (None, None, None,   0           ['bnorm_79[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_84 (Conv2D)               (None, None, None,   131072      ['leaky_79[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_84 (BatchNormalization)  (None, None, None,   1024        ['conv_84[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_84 (LeakyReLU)           (None, None, None,   0           ['bnorm_84[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, None, None,   0          ['leaky_84[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, None, None,   0           ['up_sampling2d_1[0][0]',        \n",
      "                                768)                              'add_19[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_87 (Conv2D)               (None, None, None,   196608      ['concatenate_1[0][0]']          \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_87 (BatchNormalization)  (None, None, None,   1024        ['conv_87[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_87 (LeakyReLU)           (None, None, None,   0           ['bnorm_87[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_88 (Conv2D)               (None, None, None,   1179648     ['leaky_87[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_88 (BatchNormalization)  (None, None, None,   2048        ['conv_88[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_88 (LeakyReLU)           (None, None, None,   0           ['bnorm_88[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_89 (Conv2D)               (None, None, None,   131072      ['leaky_88[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_89 (BatchNormalization)  (None, None, None,   1024        ['conv_89[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_89 (LeakyReLU)           (None, None, None,   0           ['bnorm_89[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_90 (Conv2D)               (None, None, None,   1179648     ['leaky_89[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_90 (BatchNormalization)  (None, None, None,   2048        ['conv_90[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " leaky_90 (LeakyReLU)           (None, None, None,   0           ['bnorm_90[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_91 (Conv2D)               (None, None, None,   131072      ['leaky_90[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_91 (BatchNormalization)  (None, None, None,   1024        ['conv_91[0][0]']                \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_91 (LeakyReLU)           (None, None, None,   0           ['bnorm_91[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv_96 (Conv2D)               (None, None, None,   32768       ['leaky_91[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_96 (BatchNormalization)  (None, None, None,   512         ['conv_96[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_96 (LeakyReLU)           (None, None, None,   0           ['bnorm_96[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " up_sampling2d_2 (UpSampling2D)  (None, None, None,   0          ['leaky_96[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, None, None,   0           ['up_sampling2d_2[0][0]',        \n",
      "                                384)                              'add_11[0][0]']                 \n",
      "                                                                                                  \n",
      " conv_99 (Conv2D)               (None, None, None,   49152       ['concatenate_2[0][0]']          \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_99 (BatchNormalization)  (None, None, None,   512         ['conv_99[0][0]']                \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_99 (LeakyReLU)           (None, None, None,   0           ['bnorm_99[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_100 (Conv2D)              (None, None, None,   294912      ['leaky_99[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_100 (BatchNormalization)  (None, None, None,   1024       ['conv_100[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_100 (LeakyReLU)          (None, None, None,   0           ['bnorm_100[0][0]']              \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_101 (Conv2D)              (None, None, None,   32768       ['leaky_100[0][0]']              \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_101 (BatchNormalization)  (None, None, None,   512        ['conv_101[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_101 (LeakyReLU)          (None, None, None,   0           ['bnorm_101[0][0]']              \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_102 (Conv2D)              (None, None, None,   294912      ['leaky_101[0][0]']              \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_102 (BatchNormalization)  (None, None, None,   1024       ['conv_102[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_102 (LeakyReLU)          (None, None, None,   0           ['bnorm_102[0][0]']              \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_103 (Conv2D)              (None, None, None,   32768       ['leaky_102[0][0]']              \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " bnorm_103 (BatchNormalization)  (None, None, None,   512        ['conv_103[0][0]']               \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " leaky_103 (LeakyReLU)          (None, None, None,   0           ['bnorm_103[0][0]']              \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " conv_80 (Conv2D)               (None, None, None,   4718592     ['leaky_79[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " conv_92 (Conv2D)               (None, None, None,   1179648     ['leaky_91[0][0]']               \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " conv_104 (Conv2D)              (None, None, None,   294912      ['leaky_103[0][0]']              \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " bnorm_80 (BatchNormalization)  (None, None, None,   4096        ['conv_80[0][0]']                \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " bnorm_92 (BatchNormalization)  (None, None, None,   2048        ['conv_92[0][0]']                \n",
      "                                512)                                                              \n",
      "                                                                                                  \n",
      " bnorm_104 (BatchNormalization)  (None, None, None,   1024       ['conv_104[0][0]']               \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " leaky_80 (LeakyReLU)           (None, None, None,   0           ['bnorm_80[0][0]']               \n",
      "                                1024)                                                             \n",
      "                                                                                                  \n",
      " leaky_92 (LeakyReLU)           (None, None, None,   0           ['bnorm_92[0][0]']               \n",
      "                                512)                                                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " leaky_104 (LeakyReLU)          (None, None, None,   0           ['bnorm_104[0][0]']              \n",
      "                                256)                                                              \n",
      "                                                                                                  \n",
      " conv_81 (Conv2D)               (None, None, None,   261375      ['leaky_80[0][0]']               \n",
      "                                255)                                                              \n",
      "                                                                                                  \n",
      " conv_93 (Conv2D)               (None, None, None,   130815      ['leaky_92[0][0]']               \n",
      "                                255)                                                              \n",
      "                                                                                                  \n",
      " conv_105 (Conv2D)              (None, None, None,   65535       ['leaky_104[0][0]']              \n",
      "                                255)                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62,001,757\n",
      "Trainable params: 61,949,149\n",
      "Non-trainable params: 52,608\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 14:57:14.946931: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [5000]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-06-30 14:57:14.947129: I tensorflow/core/common_runtime/executor.cc:1214] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [5000]\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(608, 608, 3)\n",
      "(608, 608, 3)\n",
      "(608, 608, 3)\n",
      "(608, 608, 3)\n",
      "(608, 608, 3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************\n",
      "calibrate...\n",
      "\n",
      "calibrate: loading existing file Yolo_files/YoloV3_calibrated.tflite\n",
      "Do you want to reset df3.csv? yes/*   yes\n",
      "Total layers:179  Convs:75  number of cases:67525\n",
      "case:\n",
      "(0, 1, 2)\n",
      "['model_1/bnorm_0/FusedBatchNormV3;model_1/conv_2/Conv2D;model_1/conv_0/Conv2D', 'model_1/leaky_0/LeakyRelu', 'model_1/zero_padding2d_1/Pad', 'model_1/bnorm_1/FusedBatchNormV3;model_1/conv_9/Conv2D;model_1/conv_1/Conv2D', 'model_1/leaky_1/LeakyRelu', 'model_1/bnorm_2/FusedBatchNormV3;model_1/conv_2/Conv2D', 'model_1/leaky_2/LeakyRelu']\n",
      "7 172 179\n",
      "\n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "\n",
      "\n",
      "Case:0/67525\n",
      "quantizing conv layers (0, 1, 2)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************\n",
      "explore combination...\n",
      "\n",
      "explore_combinations: producing file /home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 2).tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 2).tflite Quantization finished time: 7.489067077636719\n",
      "cmd arguments are:['--num_threads', '64', '--model_path', '/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 2).tflite', '--pkl_name', '(0, 1, 2).pkl']\n",
      "Initialization time cost: 0.095821s\n",
      "number of threads: 64\n",
      "load eval model time cost: 0.056661s\n",
      "number of images per thread: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Eval model thread 0:   1%|         | 1/77 [00:04<05:35,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.278090s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   3%|         | 2/77 [00:06<04:06,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.487528s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   4%|         | 3/77 [00:11<04:34,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.973535s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   5%|         | 4/77 [00:14<04:27,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.482889s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   6%|         | 5/77 [00:17<04:05,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.720057s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   8%|         | 6/77 [00:22<04:25,  3.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.230060s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   9%|         | 7/77 [00:26<04:31,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.019107s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  10%|         | 8/77 [00:29<04:10,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.012089s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  12%|        | 9/77 [00:34<04:41,  4.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 5.176476s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  13%|        | 10/77 [00:38<04:33,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.733276s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  14%|        | 11/77 [00:42<04:31,  4.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.992808s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  16%|        | 12/77 [00:45<04:03,  3.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.767777s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  17%|        | 13/77 [00:48<03:43,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.784220s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  18%|        | 14/77 [00:51<03:33,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.993618s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  19%|        | 15/77 [00:54<03:15,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.509998s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  21%|        | 16/77 [00:58<03:28,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.927446s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  22%|       | 17/77 [01:01<03:28,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.498758s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  23%|       | 18/77 [01:05<03:34,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.855379s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  25%|       | 19/77 [01:09<03:21,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.973503s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  26%|       | 20/77 [01:11<03:05,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.549434s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  27%|       | 21/77 [01:17<03:46,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 5.768103s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  29%|       | 22/77 [01:21<03:37,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.607675s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  30%|       | 23/77 [01:24<03:21,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.123450s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  31%|       | 24/77 [01:27<02:59,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.514134s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  32%|      | 25/77 [01:30<02:57,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.385237s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  34%|      | 26/77 [01:34<03:03,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.924647s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  35%|      | 27/77 [01:37<02:45,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.519444s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  36%|      | 28/77 [01:40<02:38,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.983924s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  38%|      | 29/77 [01:43<02:33,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.032826s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  39%|      | 30/77 [01:47<02:41,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.850916s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  40%|      | 31/77 [01:49<02:24,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.397483s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  42%|     | 32/77 [01:53<02:23,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.241293s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  43%|     | 33/77 [01:55<02:12,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.519610s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  44%|     | 34/77 [01:59<02:15,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.430957s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  45%|     | 35/77 [02:01<02:04,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.415179s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  47%|     | 36/77 [02:04<02:00,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.743664s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  48%|     | 37/77 [02:07<01:53,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.506643s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  49%|     | 38/77 [02:09<01:46,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.440012s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  51%|     | 39/77 [02:12<01:45,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.818416s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  52%|    | 40/77 [02:15<01:41,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.614852s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  53%|    | 41/77 [02:17<01:37,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.583450s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  55%|    | 42/77 [02:20<01:33,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.481488s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  56%|    | 43/77 [02:24<01:39,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.480780s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  57%|    | 44/77 [02:26<01:34,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.684922s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  58%|    | 45/77 [02:29<01:31,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.707868s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  60%|    | 46/77 [02:33<01:41,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.151590s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  61%|    | 47/77 [02:36<01:35,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.959590s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  62%|   | 48/77 [02:39<01:29,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.693731s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  64%|   | 49/77 [02:42<01:27,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.180482s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  65%|   | 50/77 [02:45<01:21,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.697626s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  66%|   | 51/77 [02:48<01:19,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.107690s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  68%|   | 52/77 [02:51<01:14,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.727629s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  69%|   | 53/77 [02:54<01:13,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.100950s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  70%|   | 54/77 [02:57<01:06,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.329237s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  71%|  | 55/77 [03:00<01:02,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.692660s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  73%|  | 56/77 [03:02<00:54,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.008029s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  74%|  | 57/77 [03:04<00:51,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.414864s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  75%|  | 58/77 [03:07<00:48,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.374411s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  77%|  | 59/77 [03:09<00:45,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.512707s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  78%|  | 60/77 [03:12<00:42,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.435238s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  79%|  | 61/77 [03:14<00:39,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.309171s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  81%|  | 62/77 [03:16<00:36,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.376639s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  82%| | 63/77 [03:19<00:34,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.505292s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  83%| | 64/77 [03:21<00:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.325615s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  84%| | 65/77 [03:24<00:31,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.014635s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  86%| | 66/77 [03:27<00:28,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.314546s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  87%| | 67/77 [03:29<00:25,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.622332s\n",
      "post time cost: 0.002019s\n",
      "Inference time for thread 30 cost: 209.910753s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  88%| | 68/77 [03:32<00:22,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.384469s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  90%| | 69/77 [03:34<00:20,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.319670s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  91%| | 70/77 [03:37<00:17,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.512692s\n",
      "post time cost: 0.011843s\n",
      "Inference time for thread 48 cost: 217.562227s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  92%|| 71/77 [03:39<00:14,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.009208s\n",
      "Inference time for thread 5 cost: 219.533286s\n",
      "inference time cost: 2.221831s\n",
      "post time cost: 0.009270s\n",
      "Inference time for thread 51 cost: 221.207757s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  94%|| 72/77 [03:41<00:12,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.302782s\n",
      "post time cost: 0.002119s\n",
      "Inference time for thread 14 cost: 221.895718s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  95%|| 73/77 [03:44<00:09,  2.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.653191s\n",
      "post time cost: 0.001968s\n",
      "Inference time for thread 42 cost: 224.702431s\n",
      "post time cost: 0.001538s\n",
      "Inference time for thread 6 cost: 224.938522s\n",
      "post time cost: 0.039328s\n",
      "Inference time for thread 19 cost: 225.681862s\n",
      "post time cost: 0.017143s\n",
      "Inference time for thread 10 cost: 225.894140s\n",
      "post time cost: 0.009377s\n",
      "Inference time for thread 20 cost: 225.915986s\n",
      "post time cost: 0.013805s\n",
      "Inference time for thread 17 cost: 226.153675s\n",
      "post time cost: 0.007012s\n",
      "Inference time for thread 41 cost: 226.204934s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  96%|| 74/77 [03:46<00:07,  2.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.094534s\n",
      "post time cost: 0.013176s\n",
      "Inference time for thread 60 cost: 226.820719s\n",
      "post time cost: 0.004216s\n",
      "Inference time for thread 16 cost: 228.153920s\n",
      "post time cost: 0.009070s\n",
      "Inference time for thread 1 cost: 228.383837s\n",
      "post time cost: 0.001710s\n",
      "Inference time for thread 36 cost: 228.286448s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  97%|| 75/77 [03:48<00:04,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 1.929994s\n",
      "post time cost: 0.003891s\n",
      "Inference time for thread 21 cost: 228.990302s\n",
      "post time cost: 0.003479s\n",
      "Inference time for thread 23 cost: 229.041909s\n",
      "post time cost: 0.001804s\n",
      "Inference time for thread 34 cost: 229.049401s\n",
      "post time cost: 0.003049s\n",
      "Inference time for thread 12 cost: 230.620522s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  99%|| 76/77 [03:50<00:02,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.128077s\n",
      "post time cost: 0.001895s\n",
      "Inference time for thread 13 cost: 230.887866s\n",
      "post time cost: 0.002803s\n",
      "Inference time for thread 32 cost: 231.493956s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model thread 0: 100%|| 77/77 [03:52<00:00,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 1.872597s\n",
      "post time cost: 0.003936s\n",
      "Inference time for thread 0 cost: 232.932404s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.004634s\n",
      "Inference time for thread 18 cost: 233.738188s\n",
      "post time cost: 0.002806s\n",
      "Inference time for thread 15 cost: 235.240945s\n",
      "post time cost: 0.003489s\n",
      "Inference time for thread 56 cost: 235.269096s\n",
      "post time cost: 0.004282s\n",
      "Inference time for thread 4 cost: 235.873906s\n",
      "post time cost: 0.007626s\n",
      "Inference time for thread 22 cost: 236.245279s\n",
      "post time cost: 0.002803s\n",
      "Inference time for thread 58 cost: 236.074820s\n",
      "post time cost: 0.003257s\n",
      "Inference time for thread 9 cost: 237.103673s\n",
      "post time cost: 0.002893s\n",
      "Inference time for thread 61 cost: 237.011082s\n",
      "post time cost: 0.002950s\n",
      "Inference time for thread 55 cost: 237.201056s\n",
      "post time cost: 0.004242s\n",
      "Inference time for thread 53 cost: 237.420290s\n",
      "post time cost: 0.002156s\n",
      "Inference time for thread 29 cost: 239.187421s\n",
      "post time cost: 0.003587s\n",
      "Inference time for thread 40 cost: 240.362065s\n",
      "post time cost: 0.003484s\n",
      "Inference time for thread 46 cost: 241.715267s\n",
      "post time cost: 0.002648s\n",
      "Inference time for thread 26 cost: 241.912205s\n",
      "post time cost: 0.002793s\n",
      "Inference time for thread 2 cost: 242.937650s\n",
      "post time cost: 0.003862s\n",
      "Inference time for thread 39 cost: 242.779640s\n",
      "post time cost: 0.003327s\n",
      "Inference time for thread 47 cost: 242.829319s\n",
      "post time cost: 0.002664s\n",
      "Inference time for thread 43 cost: 243.212131s\n",
      "post time cost: 0.002239s\n",
      "Inference time for thread 31 cost: 243.350149s\n",
      "post time cost: 0.001941s\n",
      "Inference time for thread 24 cost: 243.411662s\n",
      "post time cost: 0.001948s\n",
      "Inference time for thread 44 cost: 243.497724s\n",
      "post time cost: 0.002692s\n",
      "Inference time for thread 7 cost: 243.760041s\n",
      "post time cost: 0.002818s\n",
      "Inference time for thread 49 cost: 243.563890s\n",
      "post time cost: 0.002069s\n",
      "Inference time for thread 33 cost: 244.470310s\n",
      "post time cost: 0.002430s\n",
      "Inference time for thread 45 cost: 245.936596s\n",
      "post time cost: 0.001348s\n",
      "Inference time for thread 50 cost: 246.423186s\n",
      "post time cost: 0.003200s\n",
      "Inference time for thread 11 cost: 247.195686s\n",
      "post time cost: 0.002917s\n",
      "Inference time for thread 59 cost: 246.809361s\n",
      "post time cost: 0.003314s\n",
      "Inference time for thread 54 cost: 248.163733s\n",
      "post time cost: 0.002570s\n",
      "Inference time for thread 28 cost: 249.677441s\n",
      "post time cost: 0.002946s\n",
      "Inference time for thread 8 cost: 250.273412s\n",
      "post time cost: 0.002319s\n",
      "Inference time for thread 3 cost: 250.718290s\n",
      "post time cost: 0.003817s\n",
      "Inference time for thread 62 cost: 250.509480s\n",
      "post time cost: 0.001638s\n",
      "Inference time for thread 25 cost: 251.728188s\n",
      "post time cost: 0.003484s\n",
      "Inference time for thread 27 cost: 255.389973s\n",
      "post time cost: 0.005048s\n",
      "Inference time for thread 52 cost: 259.043608s\n",
      "post time cost: 0.003735s\n",
      "Inference time for thread 57 cost: 259.414776s\n",
      "post time cost: 0.004412s\n",
      "Inference time for thread 38 cost: 259.826888s\n",
      "post time cost: 0.003403s\n",
      "Inference time for thread 63 cost: 272.972661s\n",
      "post time cost: 0.003840s\n",
      "Inference time for thread 37 cost: 280.269394s\n",
      "post time cost: 0.002725s\n",
      "Inference time for thread 35 cost: 288.110069s\n",
      "Inference time: 288.335083s\n",
      "Inference time cost: 288.335152s\n",
      "Thread 0 computing mAP for ['hair drier', 'toaster', 'bear', 'microwave', 'snowboard', 'fire hydrant', 'parking meter', 'frisbee', 'scissors', 'aeroplane', 'cat', 'stop sign', 'toilet', 'mouse', 'hot dog', 'toothbrush', 'elephant', 'keyboard', 'baseball bat', 'train', 'kite', 'giraffe', 'teddy bear', 'dog', 'sandwich', 'laptop', 'refrigerator', 'tennis racket', 'bus', 'skateboard', 'oven', 'baseball glove', 'bed', 'zebra', 'horse', 'pizza', 'donut', 'tvmonitor', 'cow', 'sofa', 'fork', 'suitcase', 'motorbike', 'orange', 'wine glass', 'skis', 'surfboard', 'sports ball', 'cake', 'sink', 'vase', 'sheep', 'truck', 'apple', 'clock', 'remote', 'umbrella', 'tie', 'bicycle', 'knife', 'spoon', 'boat', 'banana', 'backpack', 'broccoli', 'bird', 'cell phone', 'pottedplant', 'bowl', 'carrot', 'cup', 'bench', 'traffic light', 'diningtable', 'handbag', 'bottle', 'book', 'car', 'chair', 'person']\n",
      "Compute AP for thread 0 time: 65.40199446678162\n",
      "Compute APs time: 65.409918s\n",
      "\n",
      "Pascal VOC AP evaluation\n",
      "mAP@IoU=0.50 result: 68.695526\n",
      "mPrec@IoU=0.50 result: 17.794970\n",
      "mRec@IoU=0.50 result: 82.500567\n",
      "reduc time : 0.0013096332550048828\n",
      "Total time: 354.875823s\n",
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 2).tflite Evaluation finished time: 362.3650951385498\n",
      "case:\n",
      "(0, 1, 3)\n",
      "['model_1/bnorm_0/FusedBatchNormV3;model_1/conv_2/Conv2D;model_1/conv_0/Conv2D', 'model_1/leaky_0/LeakyRelu', 'model_1/zero_padding2d_1/Pad', 'model_1/bnorm_1/FusedBatchNormV3;model_1/conv_9/Conv2D;model_1/conv_1/Conv2D', 'model_1/leaky_1/LeakyRelu', 'model_1/bnorm_3/FusedBatchNormV3;model_1/conv_9/Conv2D;model_1/conv_3/Conv2D', 'model_1/leaky_3/LeakyRelu', 'model_1/add_1/add', 'model_1/zero_padding2d_2/Pad']\n",
      "9 170 179\n",
      "\n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "\n",
      "\n",
      "Case:1/67525\n",
      "quantizing conv layers (0, 1, 3)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************\n",
      "explore combination...\n",
      "\n",
      "explore_combinations: producing file /home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 3).tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 3).tflite Quantization finished time: 7.407671689987183\n",
      "cmd arguments are:['--num_threads', '64', '--model_path', '/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 3).tflite', '--pkl_name', '(0, 1, 3).pkl']\n",
      "Initialization time cost: 0.103045s\n",
      "number of threads: 64\n",
      "load eval model time cost: 0.002049s\n",
      "number of images per thread: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model thread 0:   1%|         | 1/77 [00:05<06:46,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 5.322838s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   3%|         | 2/77 [00:08<05:08,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.175639s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   4%|         | 3/77 [00:12<05:07,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.103680s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   5%|         | 4/77 [00:16<04:41,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.237034s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   6%|         | 5/77 [00:19<04:19,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.882322s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   8%|         | 6/77 [00:24<04:42,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.547092s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   9%|         | 7/77 [00:28<04:39,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.946288s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  10%|         | 8/77 [00:31<04:26,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.453417s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  12%|        | 9/77 [00:36<04:41,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.602529s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  13%|        | 10/77 [00:39<04:13,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.858132s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  14%|        | 11/77 [00:42<03:54,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.948426s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  16%|        | 12/77 [00:44<03:30,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.487996s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  17%|        | 13/77 [00:47<03:19,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.842015s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  18%|        | 14/77 [00:50<03:10,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.670002s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  19%|        | 15/77 [00:53<02:59,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.561313s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  21%|        | 16/77 [00:56<03:05,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.342546s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  22%|       | 17/77 [01:00<03:14,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.662056s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  23%|       | 18/77 [01:02<03:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.459061s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  25%|       | 19/77 [01:06<03:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.235866s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  26%|       | 20/77 [01:08<02:50,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.590717s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  27%|       | 21/77 [01:15<03:42,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 6.208382s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  29%|       | 22/77 [01:18<03:35,  3.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.685267s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  30%|       | 23/77 [01:21<03:11,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.588757s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  31%|       | 24/77 [01:24<03:02,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.143170s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  32%|      | 25/77 [01:28<02:57,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.225658s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  34%|      | 26/77 [01:31<02:55,  3.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.467870s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  35%|      | 27/77 [01:34<02:41,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.728849s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  36%|      | 28/77 [01:37<02:32,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.823108s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  38%|      | 29/77 [01:39<02:22,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.622788s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  39%|      | 30/77 [01:42<02:16,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.688350s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  40%|      | 31/77 [01:45<02:07,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.438370s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  42%|     | 32/77 [01:48<02:08,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.041062s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  43%|     | 33/77 [01:50<02:04,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.698122s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  44%|     | 34/77 [01:53<02:03,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.897774s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  45%|     | 35/77 [01:56<02:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.824470s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  47%|     | 36/77 [01:59<01:56,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.734432s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  48%|     | 37/77 [02:02<01:50,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.480825s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  49%|     | 38/77 [02:05<01:51,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.049427s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  51%|     | 39/77 [02:07<01:46,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.667938s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  52%|    | 40/77 [02:10<01:44,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.810449s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  53%|    | 41/77 [02:13<01:44,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.031826s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  55%|    | 42/77 [02:16<01:36,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.358589s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  56%|    | 43/77 [02:20<01:47,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.046217s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  57%|    | 44/77 [02:22<01:39,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.607554s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  58%|    | 45/77 [02:26<01:36,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.027428s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  60%|    | 46/77 [02:28<01:32,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.863988s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  61%|    | 47/77 [02:31<01:29,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.916826s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  62%|   | 48/77 [02:34<01:26,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.918236s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  64%|   | 49/77 [02:38<01:26,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.276350s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  65%|   | 50/77 [02:40<01:19,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.579070s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  66%|   | 51/77 [02:43<01:15,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.729495s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  68%|   | 52/77 [02:46<01:08,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.355720s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  69%|   | 53/77 [02:48<01:06,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.812026s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  70%|   | 54/77 [02:51<01:05,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.941591s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  71%|  | 55/77 [02:54<01:01,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.621702s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  73%|  | 56/77 [02:57<00:57,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.584021s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  74%|  | 57/77 [02:59<00:53,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.521487s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  75%|  | 58/77 [03:02<00:53,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.144204s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  77%|  | 59/77 [03:05<00:49,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.523522s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  78%|  | 60/77 [03:07<00:45,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.550981s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  79%|  | 61/77 [03:10<00:42,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.488598s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  81%|  | 62/77 [03:13<00:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.751619s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  82%| | 63/77 [03:16<00:38,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.737271s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  83%| | 64/77 [03:18<00:35,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.661085s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  84%| | 65/77 [03:21<00:33,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.959623s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  86%| | 66/77 [03:24<00:29,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.375340s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  87%| | 67/77 [03:27<00:28,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.346452s\n",
      "post time cost: 0.001820s\n",
      "Inference time for thread 6 cost: 208.832420s\n",
      "post time cost: 0.002676s\n",
      "Inference time for thread 34 cost: 209.465246s\n",
      "post time cost: 0.002308s\n",
      "Inference time for thread 48 cost: 209.532655s\n",
      "inference time cost: 2.349316s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  88%| | 68/77 [03:30<00:25,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.000828s\n",
      "Inference time for thread 30 cost: 211.810428s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  90%| | 69/77 [03:32<00:21,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.001549s\n",
      "Inference time for thread 14 cost: 212.472370s\n",
      "inference time cost: 2.459598s\n",
      "post time cost: 0.002135s\n",
      "Inference time for thread 42 cost: 212.837666s\n",
      "post time cost: 0.001594s\n",
      "Inference time for thread 5 cost: 213.451572s\n",
      "post time cost: 0.002681s\n",
      "Inference time for thread 20 cost: 214.926301s\n",
      "post time cost: 0.002687s\n",
      "Inference time for thread 17 cost: 215.095248s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  91%| | 70/77 [03:35<00:19,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.696556s\n",
      "post time cost: 0.002427s\n",
      "Inference time for thread 60 cost: 215.283655s\n",
      "post time cost: 0.002816s\n",
      "Inference time for thread 9 cost: 215.998225s\n",
      "post time cost: 0.003737s\n",
      "Inference time for thread 10 cost: 216.990539s\n",
      "post time cost: 0.002597s\n",
      "Inference time for thread 21 cost: 216.944594s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  92%|| 71/77 [03:37<00:15,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.300574s\n",
      "post time cost: 0.002210s\n",
      "Inference time for thread 50 cost: 219.074212s\n",
      "post time cost: 0.003287s\n",
      "Inference time for thread 43 cost: 219.518681s\n",
      "post time cost: 0.003091s\n",
      "Inference time for thread 32 cost: 219.768632s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  94%|| 72/77 [03:40<00:12,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.426953s\n",
      "post time cost: 0.001913s\n",
      "Inference time for thread 16 cost: 220.185654s\n",
      "post time cost: 0.002540s\n",
      "Inference time for thread 15 cost: 220.403574s\n",
      "post time cost: 0.001115s\n",
      "Inference time for thread 36 cost: 220.247598s\n",
      "post time cost: 0.002922s\n",
      "Inference time for thread 19 cost: 220.522110s\n",
      "post time cost: 0.003000s\n",
      "Inference time for thread 56 cost: 220.336953s\n",
      "post time cost: 0.002351s\n",
      "Inference time for thread 58 cost: 220.617601s\n",
      "post time cost: 0.003407s\n",
      "Inference time for thread 25 cost: 221.245565s\n",
      "post time cost: 0.003287s\n",
      "Inference time for thread 29 cost: 221.343543s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  95%|| 73/77 [03:42<00:09,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.001518s\n",
      "Inference time for thread 23 cost: 221.992324s\n",
      "post time cost: 0.003699s\n",
      "Inference time for thread 4 cost: 222.174905s\n",
      "inference time cost: 2.056876s\n",
      "post time cost: 0.002087s\n",
      "Inference time for thread 51 cost: 222.271763s\n",
      "post time cost: 0.001388s\n",
      "Inference time for thread 13 cost: 223.381382s\n",
      "post time cost: 0.002821s\n",
      "Inference time for thread 26 cost: 223.484446s\n",
      "post time cost: 0.003122s\n",
      "Inference time for thread 27 cost: 223.622962s\n",
      "post time cost: 0.002224s\n",
      "Inference time for thread 28 cost: 223.841226s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  96%|| 74/77 [03:44<00:06,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.022396s\n",
      "post time cost: 0.001821s\n",
      "Inference time for thread 7 cost: 224.478318s\n",
      "post time cost: 0.003049s\n",
      "Inference time for thread 49 cost: 224.201302s\n",
      "post time cost: 0.003294s\n",
      "Inference time for thread 47 cost: 224.468091s\n",
      "post time cost: 0.002402s\n",
      "Inference time for thread 12 cost: 225.238319s\n",
      "post time cost: 0.004153s\n",
      "Inference time for thread 33 cost: 225.245751s\n",
      "post time cost: 0.002914s\n",
      "Inference time for thread 39 cost: 225.338748s\n",
      "post time cost: 0.001673s\n",
      "Inference time for thread 61 cost: 225.189583s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  97%|| 75/77 [03:46<00:04,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 1.869726s\n",
      "post time cost: 0.003244s\n",
      "Inference time for thread 53 cost: 226.063177s\n",
      "post time cost: 0.003514s\n",
      "Inference time for thread 1 cost: 226.599810s\n",
      "post time cost: 0.003416s\n",
      "Inference time for thread 46 cost: 226.743530s\n",
      "post time cost: 0.002810s\n",
      "Inference time for thread 59 cost: 226.625464s\n",
      "post time cost: 0.002554s\n",
      "Inference time for thread 2 cost: 227.209049s\n",
      "post time cost: 0.003439s\n",
      "Inference time for thread 11 cost: 227.348209s\n",
      "post time cost: 0.001709s\n",
      "Inference time for thread 31 cost: 227.185542s\n",
      "post time cost: 0.002820s\n",
      "Inference time for thread 22 cost: 227.427557s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  99%|| 76/77 [03:47<00:02,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.001776s\n",
      "Inference time for thread 41 cost: 227.360534s\n",
      "inference time cost: 1.678795s\n",
      "post time cost: 0.002848s\n",
      "Inference time for thread 18 cost: 228.304849s\n",
      "post time cost: 0.003022s\n",
      "Inference time for thread 3 cost: 228.919781s\n",
      "post time cost: 0.002028s\n",
      "Inference time for thread 44 cost: 228.591626s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model thread 0: 100%|| 77/77 [03:49<00:00,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 1.623523s\n",
      "post time cost: 0.003145s\n",
      "Inference time for thread 0 cost: 229.544540s\n",
      "post time cost: 0.002546s\n",
      "Inference time for thread 55 cost: 229.055310s\n",
      "post time cost: 0.001472s\n",
      "Inference time for thread 24 cost: 229.481132s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.005281s\n",
      "Inference time for thread 40 cost: 230.033945s\n",
      "post time cost: 0.002720s\n",
      "Inference time for thread 62 cost: 230.076410s\n",
      "post time cost: 0.003517s\n",
      "Inference time for thread 45 cost: 230.456632s\n",
      "post time cost: 0.003307s\n",
      "Inference time for thread 37 cost: 231.022607s\n",
      "post time cost: 0.002524s\n",
      "Inference time for thread 35 cost: 232.105392s\n",
      "post time cost: 0.006670s\n",
      "Inference time for thread 8 cost: 234.311845s\n",
      "post time cost: 0.003439s\n",
      "Inference time for thread 54 cost: 234.089117s\n",
      "post time cost: 0.003278s\n",
      "Inference time for thread 38 cost: 235.204442s\n",
      "post time cost: 0.003262s\n",
      "Inference time for thread 57 cost: 238.790628s\n",
      "post time cost: 0.005042s\n",
      "Inference time for thread 52 cost: 243.864317s\n",
      "post time cost: 0.004097s\n",
      "Inference time for thread 63 cost: 256.138488s\n",
      "Inference time: 256.745924s\n",
      "Inference time cost: 256.746032s\n",
      "Thread 0 computing mAP for ['hair drier', 'toaster', 'bear', 'microwave', 'snowboard', 'fire hydrant', 'parking meter', 'frisbee', 'scissors', 'aeroplane', 'cat', 'stop sign', 'toilet', 'hot dog', 'mouse', 'toothbrush', 'keyboard', 'elephant', 'giraffe', 'baseball bat', 'train', 'kite', 'dog', 'laptop', 'teddy bear', 'sandwich', 'refrigerator', 'tennis racket', 'bus', 'oven', 'skateboard', 'bed', 'baseball glove', 'zebra', 'horse', 'pizza', 'tvmonitor', 'donut', 'sofa', 'suitcase', 'fork', 'cow', 'orange', 'wine glass', 'motorbike', 'skis', 'cake', 'surfboard', 'sink', 'sports ball', 'vase', 'sheep', 'truck', 'remote', 'clock', 'umbrella', 'apple', 'tie', 'bicycle', 'knife', 'spoon', 'boat', 'banana', 'backpack', 'broccoli', 'cell phone', 'bird', 'pottedplant', 'bowl', 'carrot', 'cup', 'bench', 'traffic light', 'diningtable', 'handbag', 'bottle', 'book', 'chair', 'car', 'person']\n",
      "Compute AP for thread 0 time: 63.159356355667114\n",
      "Compute APs time: 63.166773s\n",
      "\n",
      "Pascal VOC AP evaluation\n",
      "mAP@IoU=0.50 result: 68.461504\n",
      "mPrec@IoU=0.50 result: 17.743236\n",
      "mRec@IoU=0.50 result: 82.206995\n",
      "reduc time : 0.0008034706115722656\n",
      "Total time: 321.103234s\n",
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 3).tflite Evaluation finished time: 328.51126742362976\n",
      "case:\n",
      "(0, 1, 4)\n",
      "['model_1/bnorm_0/FusedBatchNormV3;model_1/conv_2/Conv2D;model_1/conv_0/Conv2D', 'model_1/leaky_0/LeakyRelu', 'model_1/zero_padding2d_1/Pad', 'model_1/bnorm_1/FusedBatchNormV3;model_1/conv_9/Conv2D;model_1/conv_1/Conv2D', 'model_1/leaky_1/LeakyRelu', 'model_1/bnorm_5/FusedBatchNormV3;model_1/conv_103/Conv2D;model_1/conv_5/Conv2D', 'model_1/leaky_5/LeakyRelu']\n",
      "7 172 179\n",
      "\n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "\n",
      "\n",
      "Case:2/67525\n",
      "quantizing conv layers (0, 1, 4)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************\n",
      "explore combination...\n",
      "\n",
      "explore_combinations: producing file /home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 4).tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 4).tflite Quantization finished time: 7.432648181915283\n",
      "cmd arguments are:['--num_threads', '64', '--model_path', '/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 4).tflite', '--pkl_name', '(0, 1, 4).pkl']\n",
      "Initialization time cost: 0.108636s\n",
      "number of threads: 64\n",
      "load eval model time cost: 0.002082s\n",
      "number of images per thread: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model thread 0:   1%|         | 1/77 [00:04<05:26,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.276761s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   3%|         | 2/77 [00:06<04:00,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.390996s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   4%|         | 3/77 [00:10<04:22,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.847888s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   5%|         | 4/77 [00:14<04:17,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.399402s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   6%|         | 5/77 [00:17<04:10,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.224367s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   8%|         | 6/77 [00:22<04:35,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.554443s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   9%|         | 7/77 [00:25<04:26,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.498521s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  10%|         | 8/77 [00:28<04:01,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.675351s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  12%|        | 9/77 [00:32<04:06,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.769851s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  13%|        | 10/77 [00:35<03:57,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.337018s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  14%|        | 11/77 [00:40<04:04,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.981963s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  16%|        | 12/77 [00:42<03:30,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.076519s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  17%|        | 13/77 [00:45<03:24,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.045337s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  18%|        | 14/77 [00:47<03:07,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.390345s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  19%|        | 15/77 [00:50<03:05,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.952522s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  21%|        | 16/77 [00:53<02:59,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.690455s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  22%|       | 17/77 [00:56<02:53,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.647915s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  23%|       | 18/77 [00:59<02:59,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.325708s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  25%|       | 19/77 [01:02<02:47,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.487102s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  26%|       | 20/77 [01:05<02:48,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.079046s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  27%|       | 21/77 [01:08<02:50,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.153268s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  29%|       | 22/77 [01:11<02:48,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.031074s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  30%|       | 23/77 [01:16<03:11,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.579921s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  31%|       | 24/77 [01:18<02:47,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.217930s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  32%|      | 25/77 [01:21<02:39,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.765470s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  34%|      | 26/77 [01:24<02:33,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.820864s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  35%|      | 27/77 [01:27<02:29,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.875191s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  36%|      | 28/77 [01:30<02:22,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.638237s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  38%|      | 29/77 [01:32<02:12,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.348428s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  39%|      | 30/77 [01:36<02:27,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.025416s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  40%|      | 31/77 [01:39<02:16,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.456731s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  42%|     | 32/77 [01:41<02:10,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.709364s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  43%|     | 33/77 [01:45<02:11,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.101734s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  44%|     | 34/77 [01:48<02:13,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.311112s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  45%|     | 35/77 [01:51<02:05,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.697500s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  47%|     | 36/77 [01:54<02:05,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.087520s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  48%|     | 37/77 [01:56<01:53,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.229877s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  49%|     | 38/77 [02:00<01:57,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.366709s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  51%|     | 39/77 [02:03<01:54,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.944035s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  52%|    | 40/77 [02:05<01:47,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.630048s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  53%|    | 41/77 [02:08<01:43,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.738764s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  55%|    | 42/77 [02:11<01:40,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.855680s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  56%|    | 43/77 [02:15<01:46,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.695517s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  57%|    | 44/77 [02:18<01:40,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.714509s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  58%|    | 45/77 [02:20<01:35,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.839070s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  60%|    | 46/77 [02:24<01:39,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.651492s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  61%|    | 47/77 [02:27<01:30,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.557020s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  62%|   | 48/77 [02:29<01:24,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.665344s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  64%|   | 49/77 [02:33<01:23,  2.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.043523s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  65%|   | 50/77 [02:35<01:18,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.731010s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  66%|   | 51/77 [02:38<01:16,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.950086s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  68%|   | 52/77 [02:41<01:13,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.872845s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  69%|   | 53/77 [02:45<01:14,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.489332s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  70%|   | 54/77 [02:48<01:11,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.064354s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  71%|  | 55/77 [02:50<01:04,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.455238s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  73%|  | 56/77 [02:53<00:58,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.410838s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  74%|  | 57/77 [02:56<00:55,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.644995s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  75%|  | 58/77 [02:58<00:52,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.830782s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  77%|  | 59/77 [03:01<00:49,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.642503s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  78%|  | 60/77 [03:04<00:47,  2.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.810796s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  79%|  | 61/77 [03:06<00:43,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.414934s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  81%|  | 62/77 [03:09<00:40,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.696196s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  82%| | 63/77 [03:12<00:36,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.453907s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  83%| | 64/77 [03:14<00:34,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.654019s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  84%| | 65/77 [03:17<00:32,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.808372s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  86%| | 66/77 [03:20<00:29,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.469121s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  87%| | 67/77 [03:22<00:25,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.464926s\n",
      "post time cost: 0.018546s\n",
      "Inference time for thread 6 cost: 202.644597s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  88%| | 68/77 [03:25<00:23,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.728808s\n",
      "post time cost: 0.007496s\n",
      "Inference time for thread 30 cost: 207.378195s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  90%| | 69/77 [03:28<00:21,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.547197s\n",
      "post time cost: 0.043582s\n",
      "Inference time for thread 48 cost: 209.685343s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  91%| | 70/77 [03:30<00:18,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.689650s\n",
      "post time cost: 0.003302s\n",
      "Inference time for thread 10 cost: 214.056555s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  92%|| 71/77 [03:34<00:18,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.002265s\n",
      "Inference time for thread 17 cost: 214.626301s\n",
      "inference time cost: 3.969468s\n",
      "post time cost: 0.002373s\n",
      "Inference time for thread 14 cost: 215.034934s\n",
      "post time cost: 0.003016s\n",
      "Inference time for thread 42 cost: 216.121290s\n",
      "post time cost: 0.001863s\n",
      "Inference time for thread 13 cost: 216.849435s\n",
      "post time cost: 0.003288s\n",
      "Inference time for thread 20 cost: 216.895398s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  94%|| 72/77 [03:37<00:14,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.357967s\n",
      "post time cost: 0.002160s\n",
      "Inference time for thread 5 cost: 218.327672s\n",
      "post time cost: 0.001831s\n",
      "Inference time for thread 50 cost: 218.603250s\n",
      "post time cost: 0.002587s\n",
      "Inference time for thread 61 cost: 218.738091s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  95%|| 73/77 [03:39<00:10,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.202252s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  96%|| 74/77 [03:42<00:07,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.497782s\n",
      "post time cost: 0.002036s\n",
      "Inference time for thread 34 cost: 222.082709s\n",
      "post time cost: 0.003471s\n",
      "Inference time for thread 55 cost: 221.910135s\n",
      "post time cost: 0.001947s\n",
      "Inference time for thread 7 cost: 222.707037s\n",
      "post time cost: 0.001854s\n",
      "Inference time for thread 16 cost: 222.842159s\n",
      "post time cost: 0.003170s\n",
      "Inference time for thread 49 cost: 222.687763s\n",
      "post time cost: 0.001210s\n",
      "Inference time for thread 36 cost: 223.598041s\n",
      "post time cost: 0.002631s\n",
      "Inference time for thread 58 cost: 223.557433s\n",
      "post time cost: 0.002513s\n",
      "Inference time for thread 32 cost: 223.976851s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  97%|| 75/77 [03:44<00:05,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.002777s\n",
      "Inference time for thread 60 cost: 223.847841s\n",
      "inference time cost: 2.312489s\n",
      "post time cost: 0.002943s\n",
      "Inference time for thread 12 cost: 225.035133s\n",
      "post time cost: 0.004240s\n",
      "Inference time for thread 1 cost: 225.154763s\n",
      "post time cost: 0.004019s\n",
      "Inference time for thread 18 cost: 225.304502s\n",
      "post time cost: 0.002615s\n",
      "Inference time for thread 51 cost: 224.992377s\n",
      "post time cost: 0.002155s\n",
      "Inference time for thread 29 cost: 225.888756s\n",
      "post time cost: 0.002677s\n",
      "Inference time for thread 19 cost: 226.079338s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  99%|| 76/77 [03:46<00:02,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 1.914788s\n",
      "post time cost: 0.002932s\n",
      "Inference time for thread 40 cost: 226.308505s\n",
      "post time cost: 0.003660s\n",
      "Inference time for thread 22 cost: 227.141872s\n",
      "post time cost: 0.002141s\n",
      "Inference time for thread 23 cost: 227.456956s\n",
      "post time cost: 0.003472s\n",
      "Inference time for thread 45 cost: 227.323795s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model thread 0: 100%|| 77/77 [03:48<00:00,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.001382s\n",
      "Inference time for thread 24 cost: 227.992945s\n",
      "post time cost: 0.002233s\n",
      "Inference time for thread 9 cost: 228.143988s\n",
      "inference time cost: 1.912551s\n",
      "post time cost: 0.002459s\n",
      "Inference time for thread 0 cost: 228.294705s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.002791s\n",
      "Inference time for thread 21 cost: 228.430842s\n",
      "post time cost: 0.003701s\n",
      "Inference time for thread 39 cost: 228.328635s\n",
      "post time cost: 0.002261s\n",
      "Inference time for thread 41 cost: 228.674357s\n",
      "post time cost: 0.002818s\n",
      "Inference time for thread 26 cost: 229.300574s\n",
      "post time cost: 0.002509s\n",
      "Inference time for thread 4 cost: 229.606573s\n",
      "post time cost: 0.001733s\n",
      "Inference time for thread 15 cost: 230.377206s\n",
      "post time cost: 0.002062s\n",
      "Inference time for thread 56 cost: 229.977456s\n",
      "post time cost: 0.002220s\n",
      "Inference time for thread 31 cost: 230.500548s\n",
      "post time cost: 0.001652s\n",
      "Inference time for thread 43 cost: 230.447006s\n",
      "post time cost: 0.002455s\n",
      "Inference time for thread 44 cost: 230.753286s\n",
      "post time cost: 0.003495s\n",
      "Inference time for thread 2 cost: 231.311939s\n",
      "post time cost: 0.002486s\n",
      "Inference time for thread 47 cost: 231.409826s\n",
      "post time cost: 0.001879s\n",
      "Inference time for thread 25 cost: 231.689812s\n",
      "post time cost: 0.003668s\n",
      "Inference time for thread 53 cost: 231.510887s\n",
      "post time cost: 0.003089s\n",
      "Inference time for thread 11 cost: 232.097134s\n",
      "post time cost: 0.002370s\n",
      "Inference time for thread 3 cost: 232.705961s\n",
      "post time cost: 0.002467s\n",
      "Inference time for thread 33 cost: 232.731745s\n",
      "post time cost: 0.002866s\n",
      "Inference time for thread 59 cost: 234.436100s\n",
      "post time cost: 0.002169s\n",
      "Inference time for thread 28 cost: 234.817284s\n",
      "post time cost: 0.001624s\n",
      "Inference time for thread 35 cost: 235.036431s\n",
      "post time cost: 0.003982s\n",
      "Inference time for thread 52 cost: 237.226058s\n",
      "post time cost: 0.003586s\n",
      "Inference time for thread 27 cost: 238.386125s\n",
      "post time cost: 0.007427s\n",
      "Inference time for thread 46 cost: 238.447787s\n",
      "post time cost: 0.006386s\n",
      "Inference time for thread 8 cost: 239.317534s\n",
      "post time cost: 0.004195s\n",
      "Inference time for thread 62 cost: 240.826567s\n",
      "post time cost: 0.004201s\n",
      "Inference time for thread 38 cost: 242.143772s\n",
      "post time cost: 0.003416s\n",
      "Inference time for thread 57 cost: 242.092566s\n",
      "post time cost: 0.003332s\n",
      "Inference time for thread 54 cost: 242.938272s\n",
      "post time cost: 0.003298s\n",
      "Inference time for thread 37 cost: 249.195077s\n",
      "post time cost: 0.004342s\n",
      "Inference time for thread 63 cost: 262.101312s\n",
      "Inference time: 262.719779s\n",
      "Inference time cost: 262.719852s\n",
      "Thread 0 computing mAP for ['hair drier', 'toaster', 'bear', 'microwave', 'snowboard', 'fire hydrant', 'parking meter', 'frisbee', 'scissors', 'aeroplane', 'cat', 'stop sign', 'toilet', 'toothbrush', 'mouse', 'hot dog', 'elephant', 'keyboard', 'baseball bat', 'train', 'kite', 'giraffe', 'teddy bear', 'dog', 'sandwich', 'laptop', 'refrigerator', 'bus', 'tennis racket', 'oven', 'skateboard', 'baseball glove', 'bed', 'zebra', 'horse', 'pizza', 'donut', 'tvmonitor', 'cow', 'sofa', 'orange', 'suitcase', 'motorbike', 'fork', 'wine glass', 'skis', 'sports ball', 'surfboard', 'cake', 'sink', 'vase', 'sheep', 'truck', 'apple', 'clock', 'remote', 'umbrella', 'tie', 'bicycle', 'knife', 'spoon', 'boat', 'banana', 'backpack', 'broccoli', 'bird', 'pottedplant', 'cell phone', 'bowl', 'carrot', 'cup', 'bench', 'traffic light', 'diningtable', 'handbag', 'bottle', 'book', 'car', 'chair', 'person']\n",
      "Compute AP for thread 0 time: 63.593868255615234\n",
      "Compute APs time: 63.603307s\n",
      "\n",
      "Pascal VOC AP evaluation\n",
      "mAP@IoU=0.50 result: 68.850520\n",
      "mPrec@IoU=0.50 result: 17.789279\n",
      "mRec@IoU=0.50 result: 82.459579\n",
      "reduc time : 0.002937793731689453\n",
      "Total time: 327.586361s\n",
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 4).tflite Evaluation finished time: 335.0192663669586\n",
      "case:\n",
      "(0, 1, 5)\n",
      "['model_1/bnorm_0/FusedBatchNormV3;model_1/conv_2/Conv2D;model_1/conv_0/Conv2D', 'model_1/leaky_0/LeakyRelu', 'model_1/zero_padding2d_1/Pad', 'model_1/bnorm_1/FusedBatchNormV3;model_1/conv_9/Conv2D;model_1/conv_1/Conv2D', 'model_1/leaky_1/LeakyRelu', 'model_1/bnorm_6/FusedBatchNormV3;model_1/conv_9/Conv2D;model_1/conv_6/Conv2D', 'model_1/leaky_6/LeakyRelu']\n",
      "7 172 179\n",
      "\n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "\n",
      "\n",
      "Case:3/67525\n",
      "quantizing conv layers (0, 1, 5)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "***************************************************\n",
      "explore combination...\n",
      "\n",
      "explore_combinations: producing file /home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 5).tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 5).tflite Quantization finished time: 7.44399094581604\n",
      "cmd arguments are:['--num_threads', '64', '--model_path', '/home/ehsan/Accuracy/YOLOV3/Quantization/cases/(0, 1, 5).tflite', '--pkl_name', '(0, 1, 5).pkl']\n",
      "Initialization time cost: 0.103727s\n",
      "number of threads: 64\n",
      "load eval model time cost: 0.001884s\n",
      "number of images per thread: 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval model thread 0:   1%|         | 1/77 [00:04<05:16,  4.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.147749s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   3%|         | 2/77 [00:06<03:50,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.257527s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   4%|         | 3/77 [00:10<04:19,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.950381s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   5%|         | 4/77 [00:14<04:30,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.902767s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   6%|         | 5/77 [00:17<04:15,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.966648s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   8%|         | 6/77 [00:21<04:24,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.978377s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:   9%|         | 7/77 [00:25<04:17,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.367702s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  10%|         | 8/77 [00:28<03:55,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.739543s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  12%|        | 9/77 [00:32<04:06,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.964181s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  13%|        | 10/77 [00:35<03:43,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.646422s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  14%|        | 11/77 [00:38<03:39,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.201653s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  16%|        | 12/77 [00:41<03:24,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.680357s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  17%|        | 13/77 [00:43<03:11,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.580312s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  18%|        | 14/77 [00:46<03:01,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.514428s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  19%|        | 15/77 [00:48<02:51,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.449756s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  21%|        | 16/77 [00:52<03:02,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.452558s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  22%|       | 17/77 [00:55<03:06,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.281133s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  23%|       | 18/77 [00:58<03:01,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.851970s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  25%|       | 19/77 [01:01<02:48,  2.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.387545s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  26%|       | 20/77 [01:03<02:37,  2.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.306027s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  27%|       | 21/77 [01:07<02:56,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.991053s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  29%|       | 22/77 [01:10<02:51,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.995032s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  30%|       | 23/77 [01:14<02:52,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.288785s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  31%|       | 24/77 [01:17<02:46,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.963060s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  32%|      | 25/77 [01:19<02:35,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.519668s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  34%|      | 26/77 [01:23<02:46,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.804171s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  35%|      | 27/77 [01:26<02:30,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.395724s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  36%|      | 28/77 [01:29<02:30,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.143400s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  38%|      | 29/77 [01:31<02:19,  2.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.396769s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  39%|      | 30/77 [01:36<02:33,  3.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.108959s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  40%|      | 31/77 [01:39<02:28,  3.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.069222s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  42%|     | 32/77 [01:41<02:14,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.318882s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  43%|     | 33/77 [01:44<02:15,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.198134s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  44%|     | 34/77 [01:48<02:24,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.915751s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  45%|     | 35/77 [01:51<02:10,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.437688s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  47%|     | 36/77 [01:54<02:07,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.102243s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  48%|     | 37/77 [01:57<01:58,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.423823s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  49%|     | 38/77 [01:59<01:51,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.546975s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  51%|     | 39/77 [02:03<01:54,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.395685s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  52%|    | 40/77 [02:05<01:46,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.422499s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  53%|    | 41/77 [02:08<01:42,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.747712s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  55%|    | 42/77 [02:10<01:35,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.364590s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  56%|    | 43/77 [02:15<01:54,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 4.845371s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  57%|    | 44/77 [02:18<01:46,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.725856s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  58%|    | 45/77 [02:21<01:35,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.355500s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  60%|    | 46/77 [02:24<01:36,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.377167s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  61%|    | 47/77 [02:27<01:32,  3.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.875052s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  62%|   | 48/77 [02:29<01:22,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.218625s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  64%|   | 49/77 [02:32<01:18,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.688886s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  65%|   | 50/77 [02:35<01:14,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.563521s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  66%|   | 51/77 [02:38<01:12,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.906144s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  68%|   | 52/77 [02:40<01:09,  2.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.618103s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  69%|   | 53/77 [02:43<01:03,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.312816s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  70%|   | 54/77 [02:45<00:59,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.376289s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  71%|  | 55/77 [02:48<00:57,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.671238s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  73%|  | 56/77 [02:50<00:52,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.197123s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  74%|  | 57/77 [02:53<00:51,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.724631s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  75%|  | 58/77 [02:55<00:48,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.476079s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  77%|  | 59/77 [02:58<00:46,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.535997s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  78%|  | 60/77 [03:00<00:43,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.493732s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  79%|  | 61/77 [03:03<00:41,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.567141s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  81%|  | 62/77 [03:06<00:38,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.532347s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  82%| | 63/77 [03:08<00:36,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.592914s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  83%| | 64/77 [03:11<00:33,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.401402s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  84%| | 65/77 [03:14<00:32,  2.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 3.007191s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  86%| | 66/77 [03:16<00:29,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.335800s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  87%| | 67/77 [03:19<00:26,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.539599s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  88%| | 68/77 [03:22<00:24,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.754147s\n",
      "post time cost: 0.006024s\n",
      "Inference time for thread 30 cost: 203.028054s\n",
      "post time cost: 0.058711s\n",
      "Inference time for thread 13 cost: 203.321288s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  90%| | 69/77 [03:25<00:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.742656s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  91%| | 70/77 [03:28<00:21,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post time cost: 0.001888s\n",
      "Inference time for thread 5 cost: 208.834810s\n",
      "inference time cost: 3.828860s\n",
      "post time cost: 0.001705s\n",
      "Inference time for thread 6 cost: 209.112762s\n",
      "post time cost: 0.003776s\n",
      "Inference time for thread 10 cost: 210.845913s\n",
      "post time cost: 0.002721s\n",
      "Inference time for thread 61 cost: 210.361052s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  92%|| 71/77 [03:31<00:16,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 1.970680s\n",
      "post time cost: 0.001856s\n",
      "Inference time for thread 14 cost: 211.189048s\n",
      "post time cost: 0.002944s\n",
      "Inference time for thread 48 cost: 212.077639s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Eval model thread 0:  94%|| 72/77 [03:33<00:13,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time cost: 2.421895s\n",
      "post time cost: 0.002760s\n",
      "Inference time for thread 42 cost: 213.447287s\n"
     ]
    }
   ],
   "source": [
    "def generate_indexes_3(start_conv=-1,end_conv=-1):\n",
    "    ####\n",
    "    layer_stats = pd.read_csv(RESULTS_FILE)\n",
    "    all_layers=list(layer_stats[:]['tensor_name'])\n",
    "    conv_layers=[]\n",
    "    #print(all_layers)\n",
    "    for i,layer in enumerate(all_layers):\n",
    "        if 'conv' in layer or 'StatefulPartitionedCall' in layer:\n",
    "            conv_layers.append(layer)\n",
    "            \n",
    "    _n=len(conv_layers)\n",
    "    #cases=[ [ conv_layers[start:end+1] for end in range(start,_n) ] for start in range(0,_n) ]\n",
    "    #n_cases = [len(case) for case in cases ]\n",
    "    #N_cases = sum(n_cases)\n",
    "    #cases = list(itertools.combinations(conv_layers, 2))\n",
    "    _convs=list(range(len(conv_layers)))\n",
    "    #cases=list(itertools.combinations(_convs, 1))\n",
    "    #cases+=list(itertools.combinations(_convs, 2))\n",
    "    cases=list(itertools.combinations(_convs, 3))\n",
    "    N_cases = len(cases)\n",
    "    print(f'Total layers:{len(all_layers)}  Convs:{len(conv_layers)}  number of cases:{N_cases}')\n",
    "    #flatted_cases=[c for case in cases for c in case]\n",
    "    last_conv_indx=len(conv_layers)-1\n",
    "    last_conv=conv_layers[-1]\n",
    "    for i,case in enumerate(cases):\n",
    "        print(f'case:\\n{case}')\n",
    "        suspend=all_layers[:]\n",
    "        quant=[]\n",
    "        for layer in case:           \n",
    "            start_index=all_layers.index(conv_layers[layer])\n",
    "            if layer==last_conv_indx:\n",
    "                end_index=len(all_layers)-1\n",
    "            else:\n",
    "                end_index=all_layers.index(conv_layers[layer+1])\n",
    "            block=[all_layers[j] for j in range(start_index,end_index)]\n",
    "            quant+=block\n",
    "        print(quant)         \n",
    "        suspend=[elem for elem in all_layers if elem not in quant]\n",
    "        print(len(quant),len(suspend),len(all_layers))\n",
    "        \n",
    "        \n",
    "        yield i,N_cases,tuple(case),suspend           \n",
    "            \n",
    "def run_3():\n",
    "    output=os.getcwd()+\"/cases/\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    dffile=\"df3.csv\"\n",
    "    if os.path.isfile(dffile):\n",
    "        df=pd.read_csv(dffile,index_col=0)\n",
    "        #i=df.iloc[-1][0]+1\n",
    "        i=len(df)\n",
    "        print(f'Continue {dffile} from index {i}')\n",
    "    else:\n",
    "        response=input(f\"Do you want to reset {dffile}? yes/*   \")\n",
    "        if response==\"yes\" or response==\"Yes\":\n",
    "            df = pd.DataFrame(columns=[\"name\",\"mAP\"])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    \n",
    "    extracted_df=extract_one_two_from_consequtives()\n",
    "    \n",
    "    for c in generate_indexes_3():\n",
    "        \n",
    "        print(\"\\n\\n\\n*****************\\n\\n\\n\")\n",
    "        print(f'Case:{c[0]}/{c[1]}')\n",
    "        print(f'quantizing conv layers {c[2]}')\n",
    "        _name=f'{c[2]}'\n",
    "        if df[df['name']==_name].shape[0]:\n",
    "            print(\"Already evaluated...\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if extracted_df[extracted_df['layers'].astype(str)==_name].shape[0]:\n",
    "            mAP=extracted_df[extracted_df['layers'].astype(str)==_name]['mAP'].iloc[0]\n",
    "            df.loc[c[0]]=[_name,mAP]\n",
    "            df.to_csv(dffile)\n",
    "            print('extract')\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        m_name=output+_name+'.tflite'\n",
    "        p_name=_name+'.pkl'\n",
    "        \n",
    "        start_time=time.time()\n",
    "        explore_combinations2(calibrated_model,suspected_layers=c[-1],name=m_name)\n",
    "        end_time=time.time()\n",
    "        print(f\"{m_name} Quantization finished time: {end_time-start_time}\")\n",
    "        \n",
    "        mAP,APs=evaluate(model_name=m_name,pkl_name=p_name)\n",
    "        end_time=time.time()\n",
    "        print(f\"{m_name} Evaluation finished time: {end_time-start_time}\")\n",
    "        \n",
    "        os.remove(m_name)\n",
    "        df.loc[c[0]]=[_name,mAP]\n",
    "        \n",
    "        if c[0]%5==0 or True:\n",
    "            df.to_csv(dffile)\n",
    "            \n",
    "\n",
    "# # # +\n",
    "# # # # +\n",
    "#evaluate([])\n",
    "### run is for consequtive layer quantizations \n",
    "### and run_2 is for select two layer quantization\n",
    "## and run_3 for three layers\n",
    "if __name__ == \"__main__\":\n",
    "    initialize()\n",
    "    if os.path.isfile(RESULTS_FILE):\n",
    "        run_3()\n",
    "    else:\n",
    "        quantized_model=quantize(model,dataset)\n",
    "        debugger=explore(model,dataset)\n",
    "        run_debugger(debugger,RESULTS_FILE)\n",
    "        run_2()\n",
    "\n",
    "\n",
    "# # # + endofcell=\"-----\"\n",
    "# # # # # +\n",
    "\n",
    "# # # # + endofcell=\"----\"\n",
    "# # # # # # +\n",
    "def _run2():\n",
    "    ouput=os.getcwd()+\"/cases/\"\n",
    "    os.makedirs(output, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    layer_stats = pd.read_csv(RESULTS_FILE)\n",
    "    all_layers=list(layer_stats[:]['tensor_name'])\n",
    "    conv_layers=[]\n",
    "    for i,layer in enumerate(all_layers):\n",
    "        if 'conv' in layer or 'StatefulPartitionedCall' in layer:\n",
    "            conv_layers.append(layer)\n",
    "            \n",
    "    _n=len(conv_layers)\n",
    "    #cases=[ [ conv_layers[start:end+1] for end in range(start,_n) ] for start in range(0,_n) ]\n",
    "    cases=[  list(range(start,end+1))  for start in range(0,_n) for end in range(start,_n)]\n",
    "    n_cases = [len(case) for case in cases ]\n",
    "    N_cases = sum(n_cases)\n",
    "    print(f'Total layers:{len(all_layers)}  Convs:{len(conv_layers)}  number of cases:{N_cases}')\n",
    "    \n",
    "    def data_case(i):\n",
    "        Data={}\n",
    "        start_conv=cases[i][0]\n",
    "        end_conv=cases[i][-1]\n",
    "        start_index=all_layers.index(conv_layers[start_conv])\n",
    "        if end_conv==len(conv_layers)-1:\n",
    "            end_index=len(all_layers)-1\n",
    "        else:\n",
    "            end_index=all_layers.index(conv_layers[end_conv+1])\n",
    "        suspend=all_layers[0:start_index]+all_layers[end_index:]\n",
    "        _name=f'{start_conv}-{end_conv}'\n",
    "        \n",
    "        Data['start_conv']=start_conv\n",
    "        Data['end_conv']=end_conv\n",
    "        Data['suspend']=suspend\n",
    "        Data['_name']=_name\n",
    "        return Data\n",
    "        \n",
    "    Data1=data_case(0)\n",
    "    thread = threading.Thread(target=explore_combinations2,args=(calibrated_model,suspected_layers:=c[-1],name:=m_name))\n",
    "    thread.start()\n",
    "    for j in range(1,len(cases)):\n",
    "        \n",
    "        #kk=list(set(all_layers)-set(suspend))\n",
    "        print(\"\\n\\n\\n*****************\")\n",
    "        print(f'quantizing conv layers from {start_conv} to {end_conv}')\n",
    "        print(f'index {start_index} to {end_index}')\n",
    "        print(all_layers[start_index:end_index])\n",
    "        print(f'Case:{i}/{N_cases}')\n",
    "        _name=f'{start_conv}-{end_conv}'\n",
    "        m_name=output+_name+'.tflite'\n",
    "        p_name=_name+'.pkl'\n",
    "        if i==0:\n",
    "            thread = threading.Thread(target=explore_combinations2,args=(calibrated_model,suspected_layers:=c[-1],name:=m_name))\n",
    "            thread.start()\n",
    "            thread.join()\n",
    "            \n",
    "        next_start_conv=cases[i+1][0]\n",
    "        next_end_conv=cases[i+1][-1]\n",
    "        next_start_index=all_layers.index(conv_layers[next_start_conv])\n",
    "        next_end_index=all_layers.index(conv_layers[next_end_conv+1])\n",
    "        next_suspend=all_layers[0:start_index]+all_layers[end_index:]\n",
    "        \n",
    "    for c in generate_indexes():\n",
    "        threads=[]\n",
    "        start_time=time.time()\n",
    "        print(\"\\n\\n\\n*****************\\n\\n\\n\")\n",
    "        print(f'Case:{c[0]}/{c[1]}')\n",
    "        _name=f'{c[2]}-{c[3]}'\n",
    "        m_name=output+_name+'.tflite'\n",
    "        p_name=_name+'.pkl'\n",
    "        thread = threading.Thread(target=explore_combinations2,args=(calibrated_model,suspected_layers:=c[-1],name:=m_name))\n",
    "        end_time=time.time()\n",
    "        print(f\"{mname} Quantization finished time: {end_time-start_time}\")\n",
    "        ## Trigger Evaluation\n",
    "        thread = threading.Thread(target=evaluate, args=[model_name:=m_name,pkl_name:=p_name])\n",
    "        threads.append(thread)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "\n",
    "\n",
    "def generate_cases_keras():\n",
    "    all_layers=[l.name for l in model.layers]\n",
    "    conv_layers=[name for (indx,name) in enumerate(all_layers) if 'conv' in name]\n",
    "    _n=len(conv_layers)\n",
    "    cases=[ [ conv_layers[start:end+1] for end in range(start,_n) ] for start in range(0,_n) ]\n",
    "    n_cases = [len(case) for case in cases ]\n",
    "    N_cases = sum(n_cases)\n",
    "    flatted_cases=[c for case in cases for c in case]\n",
    "    #test=flatted_cases[4]\n",
    "    #t=explore_combinations2(calibrated_model,suspected_layers=test,name='ttt.tflite')\n",
    "    return flatted_cases\n",
    "\n",
    "def run_keras():\n",
    "    global calibrated_model\n",
    "    threads=[]\n",
    "    for case in flatted_cases:\n",
    "        start_time=time.time()\n",
    "        c=f'{case[0]}-{case[-1]}'\n",
    "        mname=resdir+c+'.tflite'\n",
    "        print(f\"Quantizaing layers {c} --> {mname}\")\n",
    "        explore_combinations2(calibrated_model,suspected_layers=case,name=mname)\n",
    "        end_time=time.time()\n",
    "        print(f\"{mname} Quantization finished time: {end_time-start_time}\")\n",
    "        thread = threading.Thread(target=evaluate, args=[model_name:=mname,pkl_name:=f'{c}.pkl'])\n",
    "        threads.append(thread)\n",
    "        input(\"one_run...\")\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "\n",
    "# # # # # + endofcell=\"--\"\n",
    "# -\n",
    "\n",
    "'''\n",
    "sq=selective_quantize(model,dataset,t=0.31,p=0.3)\n",
    "calibrated_model=calibrate(model,dataset)\n",
    "quantized_model=quantize(model,dataset)\n",
    "debugger=explore(model,dataset)\n",
    "run_debugger(debugger,RESULTS_FILE)\n",
    "Analyze(RESULTS_FILE_ANALYZED)\n",
    "selective_quantized=selective_quantize(model,dataset)\n",
    "calibrated_model=calibrate(model,dataset)\n",
    "debugger=explore_propogation(calibrated_model,dataset)\n",
    "run_debugger(debugger,RESULTS_FILE_Propogate)\n",
    "Analyze(RESULTS_FILE_Propogate_ANALYZED)\n",
    "calibrated_model=calibrate(model,dataset)\n",
    "selective_unquantized=explore_combinations(calibrated_model)'''\n",
    "\n",
    "\n",
    "# # # # # # # # # %%timeit -n 1 -r 1\n",
    "def ttt():\n",
    "    QuantizedName='Yolo_files/1/YoloV3_quztized.tflite'\n",
    "    model = interpreter_wrapper.Interpreter(model_path=QuantizedName)\n",
    "    model.allocate_tensors()\n",
    "    model_format = 'TFLITE'\n",
    "    model_input_shape=(608,608)\n",
    "    #Ehsan input shape correctness\n",
    "    input_details = model.get_input_details()\n",
    "    output_details = model.get_output_details()\n",
    "    input_shape = input_details[0]['shape']\n",
    "\n",
    "    n=100\n",
    "    input_shape[0]=n\n",
    "    input_shape[1] = model_input_shape[0]\n",
    "    input_shape[2] = model_input_shape[1]\n",
    "    model.resize_tensor_input(0, input_shape)\n",
    "    model.allocate_tensors()\n",
    "    print(input_shape)\n",
    "    input_shape=[n,608,608,3]\n",
    "    input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "    # Set the input tensor to the interpreter\n",
    "    model.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    # Run the model on the GPU\n",
    "    with tf.device('/gpu:1'):\n",
    "        model.invoke()\n",
    "\n",
    "    # Get the output tensor from the interpreter\n",
    "    output_data = model.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    output_data\n",
    "# ---\n",
    "# --\n",
    "# ----\n",
    "# -----\n",
    "# ------\n",
    "# -------\n",
    "\n",
    "\n",
    "a=list(range(10))\n",
    "import itertools\n",
    "cases=list(itertools.combinations(a, 1))\n",
    "cases+=list(itertools.combinations(a, 2))\n",
    "cases"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "qe",
   "language": "python",
   "name": "qe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
